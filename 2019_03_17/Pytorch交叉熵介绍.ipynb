{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reinforcement Learning 01.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXl0DnfzdhDT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D45oRClYvbQ7",
        "colab_type": "text"
      },
      "source": [
        "## LogSoftmax说明\n",
        "\n",
        "例如我们的输入是(1,1,1,1), 那么LogSoftmax就是进行如下的操作.\n",
        "\n",
        "$$\n",
        "Log(\\frac{e^{1}}{e^{1}+e^{1}+e^{1}+e^{1}}) = -1.3863\n",
        "$$\n",
        "\n",
        "我们拆开来看就是首先计算Softmax\n",
        "\n",
        "$$\n",
        "\\frac{e^{1}}{e^{1}+e^{1}+e^{1}+e^{1}} = 0.25\n",
        "$$\n",
        "\n",
        "接着对上面的结果取对数\n",
        "\n",
        "$$\n",
        "Log(0.25) = -1.3863\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnwBcWa3vd1m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "089d7ef5-c12e-4605-c596-cd4b19eb011d"
      },
      "source": [
        "ll = nn.LogSoftmax()\n",
        "input = torch.tensor([1.0,1.0,1.0,1.0])\n",
        "output = ll(input)\n",
        "print(output)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([-1.3863, -1.3863, -1.3863, -1.3863])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FVh_c4kwHSp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b69696f7-b61c-48f2-d93b-7754e185c249"
      },
      "source": [
        "torch.log(torch.tensor([0.25]))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-1.3863])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "267OAaKZy6oi",
        "colab_type": "text"
      },
      "source": [
        "## NLLLoss说明\n",
        "\n",
        "下面的例子中, target为2, 也就是表示one-hot表示为(0,0,1), 于是NLLLoss的计算如下所示.\n",
        "\n",
        "$$\n",
        "-(0*0.2+0*0.3+1*0.5)=-0.5\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kK6ZP1E1y9rb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c8f31f43-9d6c-4e5b-af0a-3cc4ce8766e6"
      },
      "source": [
        "loss = nn.NLLLoss()\n",
        "input = torch.tensor([[0.2, 0.3, 0.5]])\n",
        "target = torch.tensor([2]).long()\n",
        "output = loss(input, target)\n",
        "print(output)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(-0.5000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPKZJB93eA1j",
        "colab_type": "text"
      },
      "source": [
        "## CrossEntropy说明\n",
        "\n",
        "首先我们先不管Pytorch中是如何实现交叉熵的, 我们先自己来看一下交叉熵是如何计算的.\n",
        "\n",
        "交叉熵的计算公式如下所示:\n",
        "\n",
        "$$\n",
        "Loss = \\hat{-y_{i}}*Log(y_{i})\n",
        "$$\n",
        "\n",
        "其中,\n",
        "\n",
        "- $\\hat{-y_{i}}$是实际值\n",
        "- $y_{i}$是预测值\n",
        "\n",
        "假设现在的predict为$(\\frac{1}{4}, \\frac{1}{4}, \\frac{1}{4}, \\frac{1}{4})$, target为$(0, 0, 0, 1)$, 那么此时的交叉熵就是:\n",
        "\n",
        "$$\n",
        "-1*Log(\\frac{1}{4}) = 1.3863\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "下面我们看一下PyTorch中CrossEntropy是如何计算的. 前面说了, CrossEntropy是LogSoftmax和NLLLoss的结合.\n",
        "\n",
        "比如此时output为$(a_{1}, a_{2}, a_{3}, a_{4})$, 最后的target是${0,1,0,0}$.\n",
        "\n",
        "首先output经过LogSoftmax之后变为下面的式子.\n",
        "\n",
        "$$\n",
        "(log(\\frac{e^{a_{1}}}{\\sum_{i=1}^{4}{a_{i}}}), log(\\frac{e^{a_{2}}}{\\sum_{i=1}^{4}{a_{i}}}), log(\\frac{e^{a_{3}}}{\\sum_{i=1}^{4}{a_{i}}}), log(\\frac{e^{a_{4}}}{\\sum_{i=1}^{4}{a_{i}}}))\n",
        "$$\n",
        "\n",
        "接着计算NLLLoss\n",
        "\n",
        "$$\n",
        "-1*log(\\frac{e^{a_{2}}}{\\sum_{i=1}^{4}{a_{i}}}) = -log(\\frac{e^{a_{2}}}{\\sum_{i=1}^{4}{a_{i}}})\n",
        "$$\n",
        "\n",
        "这个结果是和上面交叉熵的定义是一样的. 我们可以想象成在LogSoftmax中, 我们不仅将原来的output转换为了概率值, 还求了log, 最后只需要和target相乘即可.\n",
        "\n",
        "----\n",
        "\n",
        "我们看一下自己上面的过程是不是和计算的结果是一样的. 只需要将CrossEntropyLoss的input设置为(1,1,1,1), 这样经过softmax之后的概率就$(\\frac{1}{4}, \\frac{1}{4}, \\frac{1}{4}, \\frac{1}{4})$, 所以这里计算得到的交叉熵应该是1.3863.\n",
        "\n",
        "下面实际运算一下"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZN3JfvsN2R7F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "76b22d34-401a-4cf8-a526-f66fa103add7"
      },
      "source": [
        "loss = nn.CrossEntropyLoss()\n",
        "input = torch.tensor([[1, 1, 1.0, 1]])\n",
        "target = torch.tensor([2]).long()\n",
        "output = loss(input, target)\n",
        "print(output)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1.3863)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlwoqJfq5yGU",
        "colab_type": "text"
      },
      "source": [
        "我们知道\n",
        "\n",
        "- 当预测的概率为(1,0,0), 实际就是(1,0,0)的时候, 交叉熵是接近0的.\n",
        "- 当预测的概率为(0,0,1), 实际就是(1,0,0)的时候, 交叉熵是接近正无穷的.\n",
        "\n",
        "下面看一下例子."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jp70yn-erns",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3a9f571a-8692-492b-d9f5-ab219d332cf0"
      },
      "source": [
        "# 预测和实际很接近\n",
        "loss = nn.CrossEntropyLoss()\n",
        "input = torch.tensor([[0, 0, 100.0]])\n",
        "target = torch.tensor([2]).long()\n",
        "output = loss(input, target)\n",
        "print(output)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lT5PQGfNhAOu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7799ed84-e32a-4026-8378-aecb2c5804d9"
      },
      "source": [
        "# 预测和实际差很远\n",
        "loss = nn.CrossEntropyLoss()\n",
        "input = torch.tensor([[100.0, 0, 0]])\n",
        "target = torch.tensor([2]).long()\n",
        "output = loss(input, target)\n",
        "print(output)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(100.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtFD6Dy16NNI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}