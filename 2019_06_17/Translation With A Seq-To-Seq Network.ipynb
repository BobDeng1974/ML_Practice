{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络实现翻译\n",
    "\n",
    "- 参考链接 : https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "\n",
    "- 论文参考链接 : https://arxiv.org/abs/1409.3215\n",
    "\n",
    "In this project we will be teaching a neural network to translate from French to English.\n",
    "\n",
    "最终实现的目标如下\n",
    "\n",
    "```python\n",
    "[KEY: > input, = target, < output]\n",
    "\n",
    "> il est en train de peindre un tableau .\n",
    "= he is painting a picture .\n",
    "< he is painting a picture .\n",
    "\n",
    "> pourquoi ne pas essayer ce vin delicieux ?\n",
    "= why not try that delicious wine ?\n",
    "< why not try that delicious wine ?\n",
    "\n",
    "> elle n est pas poete mais romanciere .\n",
    "= she is not a poet but a novelist .\n",
    "< she not not a poet but a novelist .\n",
    "\n",
    "> vous etes trop maigre .\n",
    "= you re too skinny .\n",
    "< you re all alone .\n",
    "```\n",
    "\n",
    "### 主要思想\n",
    "\n",
    "An encoder network condenses an input sequence into a vector, and a decoder network unfolds that vector into a new sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理&读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    \"\"\"word → index (word2index) and index → word (index2word) dictionaries\n",
    "       A count of each word word2count to use to later replace rare words.\n",
    "    \"\"\"\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " we will turn Unicode characters to ASCII, make everything lowercase, \n",
    " and trim most punctuation.\n",
    "\"\"\"\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i am a boy ! '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 转换为ASCII, 大写变小写, 留下重要的标点, 去掉大部分的标点\n",
    "normalizeString('I am a Boy!~$%^&')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    \"\"\"逐行读取file, 并将每行分为pair, 并做标准化\n",
    "    \"\"\"\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('./data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了加快训练的速度, 我们把句子长度最大设置为10, 同时我们过滤句子后使得其开头变为如i am, he is等词汇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[1].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i am a girl', 'i am a boy']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 会去掉单词个数超过10个的句子\n",
    "# 会去掉不是以特定开头的句子\n",
    "filterPairs([['i am a girl','i am a boy'],\n",
    "             ['how are you','how are you'],\n",
    "            ['i am a girl i am a girl i am a girl','i am a girl i am a girl']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full process for preparing the data is:\n",
    "\n",
    "- Read text file and split into lines, split lines into pairs\n",
    "- Normalize text, filter by length and content\n",
    "- Make word lists from sentences in pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    \"\"\"开始读取语言的文件\n",
    "    \"\"\"\n",
    "    # 读取文件, 返回的是句子对\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    # 过滤掉句子对中较长的句子, 和\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 10599 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 4345\n",
      "eng 2803\n"
     ]
    }
   ],
   "source": [
    "# 开始读取数据\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['il a accepte de faire le travail .', 'he s agreed to do the job .']\n"
     ]
    }
   ],
   "source": [
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': 2,\n",
       " 'm': 3,\n",
       " '.': 4,\n",
       " 'ok': 5,\n",
       " 'fat': 6,\n",
       " 'fit': 7,\n",
       " 'hit': 8,\n",
       " '!': 9,\n",
       " 'ill': 10,\n",
       " 'sad': 11,\n",
       " 'shy': 12,\n",
       " 'wet': 13,\n",
       " 'he': 14,\n",
       " 's': 15,\n",
       " 'am': 16,\n",
       " 'back': 17,\n",
       " 'bald': 18,\n",
       " 'busy': 19,\n",
       " 'calm': 20,\n",
       " 'cold': 21,\n",
       " 'done': 22,\n",
       " 'fine': 23,\n",
       " 'free': 24,\n",
       " 'full': 25,\n",
       " 'glad': 26,\n",
       " 'home': 27,\n",
       " 'late': 28,\n",
       " 'lazy': 29,\n",
       " 'okay': 30,\n",
       " 'safe': 31,\n",
       " 'sick': 32,\n",
       " 'sure': 33,\n",
       " 'tall': 34,\n",
       " 'thin': 35,\n",
       " 'tidy': 36,\n",
       " 'ugly': 37,\n",
       " 'weak': 38,\n",
       " 'well': 39,\n",
       " 'is': 40,\n",
       " 'old': 41,\n",
       " 'a': 42,\n",
       " 'dj': 43,\n",
       " 'good': 44,\n",
       " 'rich': 45,\n",
       " 'here': 46,\n",
       " 'cop': 47,\n",
       " 'man': 48,\n",
       " 'alone': 49,\n",
       " 'armed': 50,\n",
       " 'awake': 51,\n",
       " 'blind': 52,\n",
       " 'broke': 53,\n",
       " 'crazy': 54,\n",
       " 'cured': 55,\n",
       " 'drunk': 56,\n",
       " 'dying': 57,\n",
       " 'early': 58,\n",
       " 'first': 59,\n",
       " 'fussy': 60,\n",
       " 'going': 61,\n",
       " 'loyal': 62,\n",
       " 'lucky': 63,\n",
       " 'lying': 64,\n",
       " 'quiet': 65,\n",
       " 'ready': 66,\n",
       " 'right': 67,\n",
       " 'sober': 68,\n",
       " 'sorry': 69,\n",
       " 'stuck': 70,\n",
       " 'timid': 71,\n",
       " 'tired': 72,\n",
       " 'tough': 73,\n",
       " 'yours': 74,\n",
       " 'she': 75,\n",
       " 'hot': 76,\n",
       " 'we': 77,\n",
       " 're': 78,\n",
       " 'kind': 79,\n",
       " 'poor': 80,\n",
       " 'swiss': 81,\n",
       " 'smart': 82,\n",
       " 'human': 83,\n",
       " 'french': 84,\n",
       " 'korean': 85,\n",
       " 'hero': 86,\n",
       " 'liar': 87,\n",
       " 'baking': 88,\n",
       " 'better': 89,\n",
       " 'buying': 90,\n",
       " 'chubby': 91,\n",
       " 'eating': 92,\n",
       " 'famous': 93,\n",
       " 'faster': 94,\n",
       " 'flabby': 95,\n",
       " 'greedy': 96,\n",
       " 'hiding': 97,\n",
       " 'honest': 98,\n",
       " 'humble': 99,\n",
       " 'hungry': 100,\n",
       " 'immune': 101,\n",
       " 'in': 102,\n",
       " 'bed': 103,\n",
       " 'joking': 104,\n",
       " 'loaded': 105,\n",
       " 'lonely': 106,\n",
       " 'losing': 107,\n",
       " 'moving': 108,\n",
       " 'normal': 109,\n",
       " 'paying': 110,\n",
       " 'pooped': 111,\n",
       " 'rested': 112,\n",
       " 'ruined': 113,\n",
       " 'shaken': 114,\n",
       " 'single': 115,\n",
       " 'skinny': 116,\n",
       " 'sleepy': 117,\n",
       " 'sneaky': 118,\n",
       " 'strict': 119,\n",
       " 'strong': 120,\n",
       " 'thirty': 121,\n",
       " 'wasted': 122,\n",
       " 'nice': 123,\n",
       " 'are': 124,\n",
       " 'men': 125,\n",
       " 'even': 126,\n",
       " 'lost': 127,\n",
       " 'sunk': 128,\n",
       " 'you': 129,\n",
       " 'bad': 130,\n",
       " 'big': 131,\n",
       " 'fun': 132,\n",
       " 'eight': 133,\n",
       " 'hated': 134,\n",
       " 'nasty': 135,\n",
       " 'young': 136,\n",
       " 'hunk': 137,\n",
       " 'jerk': 138,\n",
       " 'nerd': 139,\n",
       " 'slob': 140,\n",
       " 'asleep': 141,\n",
       " 'coming': 142,\n",
       " 'crying': 143,\n",
       " 'faking': 144,\n",
       " 'my': 145,\n",
       " 'age': 146,\n",
       " 'not': 147,\n",
       " 'cook': 148,\n",
       " 'monk': 149,\n",
       " 'taller': 150,\n",
       " 'too': 151,\n",
       " 'finnish': 152,\n",
       " 'italian': 153,\n",
       " 'baker': 154,\n",
       " 'all': 155,\n",
       " 'set': 156,\n",
       " 'ashamed': 157,\n",
       " 'at': 158,\n",
       " 'baffled': 159,\n",
       " 'blessed': 160,\n",
       " 'careful': 161,\n",
       " 'certain': 162,\n",
       " 'chicken': 163,\n",
       " 'correct': 164,\n",
       " 'curious': 165,\n",
       " 'dancing': 166,\n",
       " 'dieting': 167,\n",
       " 'driving': 168,\n",
       " 'engaged': 169,\n",
       " 'excited': 170,\n",
       " 'fasting': 171,\n",
       " 'finicky': 172,\n",
       " 'frantic': 173,\n",
       " 'furious': 174,\n",
       " 'healthy': 175,\n",
       " 'humming': 176,\n",
       " 'luck': 177,\n",
       " 'jealous': 178,\n",
       " 'jittery': 179,\n",
       " 'kidding': 180,\n",
       " 'leaving': 181,\n",
       " 'married': 182,\n",
       " 'no': 183,\n",
       " 'fool': 184,\n",
       " 'mad': 185,\n",
       " 'on': 186,\n",
       " 'duty': 187,\n",
       " 'patient': 188,\n",
       " 'popular': 189,\n",
       " 'psyched': 190,\n",
       " 'psychic': 191,\n",
       " 'puzzled': 192,\n",
       " 'reading': 193,\n",
       " 'relaxed': 194,\n",
       " 'retired': 195,\n",
       " 'selfish': 196,\n",
       " 'serious': 197,\n",
       " 'shocked': 198,\n",
       " 'sincere': 199,\n",
       " 'sloshed': 200,\n",
       " 'so': 201,\n",
       " 'starved': 202,\n",
       " 'staying': 203,\n",
       " 'stuffed': 204,\n",
       " 'stunned': 205,\n",
       " 'talking': 206,\n",
       " 'teasing': 207,\n",
       " 'thirsty': 208,\n",
       " 'through': 209,\n",
       " 'touched': 210,\n",
       " 'unhappy': 211,\n",
       " 'unlucky': 212,\n",
       " 'wealthy': 213,\n",
       " 'winning': 214,\n",
       " 'working': 215,\n",
       " 'worried': 216,\n",
       " 'curt': 217,\n",
       " 'dead': 218,\n",
       " 'dog': 219,\n",
       " 'fox': 220,\n",
       " 'they': 221,\n",
       " 'angry': 222,\n",
       " 'bored': 223,\n",
       " 'happy': 224,\n",
       " 'saved': 225,\n",
       " 'twins': 226,\n",
       " 'cool': 227,\n",
       " 'fair': 228,\n",
       " 'nuts': 229,\n",
       " 'rude': 230,\n",
       " 'poet': 231,\n",
       " 'cranky': 232,\n",
       " 'heroic': 233,\n",
       " 'english': 234,\n",
       " 'bigot': 235,\n",
       " 'pain': 236,\n",
       " 'out': 237,\n",
       " 'now': 238,\n",
       " 'cute': 239,\n",
       " 'coward': 240,\n",
       " 'doctor': 241,\n",
       " 'farmer': 242,\n",
       " 'purist': 243,\n",
       " 'addicted': 244,\n",
       " 'ears': 245,\n",
       " 'an': 246,\n",
       " 'adult': 247,\n",
       " 'agent': 248,\n",
       " 'bleeding': 249,\n",
       " 'confused': 250,\n",
       " 'creative': 251,\n",
       " 'cultured': 252,\n",
       " 'divorced': 253,\n",
       " 'drowning': 254,\n",
       " 'eighteen': 255,\n",
       " 'faithful': 256,\n",
       " 'famished': 257,\n",
       " 'fearless': 258,\n",
       " 'fighting': 259,\n",
       " 'finished': 260,\n",
       " 'freezing': 261,\n",
       " 'grounded': 262,\n",
       " 'gullible': 263,\n",
       " 'homesick': 264,\n",
       " 'hungover': 265,\n",
       " 'paris': 266,\n",
       " 'innocent': 267,\n",
       " 'involved': 268,\n",
       " 'managing': 269,\n",
       " 'new': 270,\n",
       " 'rebel': 271,\n",
       " 'saint': 272,\n",
       " 'deaf': 273,\n",
       " 'dumb': 274,\n",
       " 'evil': 275,\n",
       " 'hurt': 276,\n",
       " 'mean': 277,\n",
       " 'off': 278,\n",
       " 'offended': 279,\n",
       " 'outraged': 280,\n",
       " 'powerful': 281,\n",
       " 'prepared': 282,\n",
       " 'punctual': 283,\n",
       " 'rational': 284,\n",
       " 'reformed': 285,\n",
       " 'reliable': 286,\n",
       " 'restless': 287,\n",
       " 'ruthless': 288,\n",
       " 'shooting': 289,\n",
       " 'sleeping': 290,\n",
       " 'speaking': 291,\n",
       " 'starving': 292,\n",
       " 'stubborn': 293,\n",
       " 'the': 294,\n",
       " 'boss': 295,\n",
       " 'thinking': 296,\n",
       " 'thorough': 297,\n",
       " 'thrilled': 298,\n",
       " 'ticklish': 299,\n",
       " 'truthful': 300,\n",
       " 'unbiased': 301,\n",
       " 'upstairs': 302,\n",
       " 'very': 303,\n",
       " 'worn': 304,\n",
       " 'sharp': 305,\n",
       " 'wrong': 306,\n",
       " 'boys': 307,\n",
       " 'cops': 308,\n",
       " 'gone': 309,\n",
       " 'mine': 310,\n",
       " 'arabs': 311,\n",
       " 'team': 312,\n",
       " 'adults': 313,\n",
       " 'war': 314,\n",
       " 'biased': 315,\n",
       " 'closed': 316,\n",
       " 'dating': 317,\n",
       " 'doomed': 318,\n",
       " 'inside': 319,\n",
       " 'trying': 320,\n",
       " 'bossy': 321,\n",
       " 'cruel': 322,\n",
       " 'fired': 323,\n",
       " 'funny': 324,\n",
       " 'gross': 325,\n",
       " 'moody': 326,\n",
       " 'naive': 327,\n",
       " 'silly': 328,\n",
       " 'upset': 329,\n",
       " 'weird': 330,\n",
       " 'british': 331,\n",
       " 'thief': 332,\n",
       " 'foolish': 333,\n",
       " 'type': 334,\n",
       " 'running': 335,\n",
       " 'skating': 336,\n",
       " 'grouch': 337,\n",
       " 'jesuit': 338,\n",
       " 'senior': 339,\n",
       " 'tycoon': 340,\n",
       " 'adorable': 341,\n",
       " 'after': 342,\n",
       " 'me': 343,\n",
       " 'annoying': 344,\n",
       " 'demented': 345,\n",
       " 'tokyo': 346,\n",
       " 'insecure': 347,\n",
       " 'studying': 348,\n",
       " 'slow': 349,\n",
       " 'your': 350,\n",
       " 'son': 351,\n",
       " 'american': 352,\n",
       " 'japanese': 353,\n",
       " 'muslim': 354,\n",
       " 'runner': 355,\n",
       " 'diabetic': 356,\n",
       " 'student': 357,\n",
       " 'teacher': 358,\n",
       " 'adaptable': 359,\n",
       " 'afraid': 360,\n",
       " 'ambitious': 361,\n",
       " 'artist': 362,\n",
       " 'orphan': 363,\n",
       " 'attentive': 364,\n",
       " 'available': 365,\n",
       " 'beautiful': 366,\n",
       " 'concerned': 367,\n",
       " 'confident': 368,\n",
       " 'contented': 369,\n",
       " 'convinced': 370,\n",
       " 'depressed': 371,\n",
       " 'desperate': 372,\n",
       " 'different': 373,\n",
       " 'disgusted': 374,\n",
       " 'easygoing': 375,\n",
       " 'exhausted': 376,\n",
       " 'forgetful': 377,\n",
       " 'tom': 378,\n",
       " 'hung': 379,\n",
       " 'over': 380,\n",
       " 'impatient': 381,\n",
       " 'important': 382,\n",
       " 'impressed': 383,\n",
       " 'impulsive': 384,\n",
       " 'boston': 385,\n",
       " 'danger': 386,\n",
       " 'intrigued': 387,\n",
       " 'just': 388,\n",
       " 'listening': 389,\n",
       " 'motivated': 390,\n",
       " 'expert': 391,\n",
       " 'fan': 392,\n",
       " 'obese': 393,\n",
       " 'short': 394,\n",
       " 'observant': 395,\n",
       " 'diet': 396,\n",
       " 'way': 397,\n",
       " 'plastered': 398,\n",
       " 'powerless': 399,\n",
       " 'realistic': 400,\n",
       " 'resentful': 401,\n",
       " 'resilient': 402,\n",
       " 'satisfied': 403,\n",
       " 'saying': 404,\n",
       " 'sensitive': 405,\n",
       " 'surprised': 406,\n",
       " 'surviving': 407,\n",
       " 'terrified': 408,\n",
       " 'uninsured': 409,\n",
       " 'unmarried': 410,\n",
       " 'voting': 411,\n",
       " 'falling': 412,\n",
       " 'twin': 413,\n",
       " 'active': 414,\n",
       " 'gentle': 415,\n",
       " 'cutie': 416,\n",
       " 'model': 417,\n",
       " 'awesome': 418,\n",
       " 'asian': 419,\n",
       " 'alive': 420,\n",
       " 'brave': 421,\n",
       " 'clean': 422,\n",
       " 'small': 423,\n",
       " 'spies': 424,\n",
       " 'there': 425,\n",
       " 'group': 426,\n",
       " 'anxious': 427,\n",
       " 'cousins': 428,\n",
       " 'friends': 429,\n",
       " 'love': 430,\n",
       " 'sync': 431,\n",
       " 'town': 432,\n",
       " 'sinking': 433,\n",
       " 'smashed': 434,\n",
       " 'special': 435,\n",
       " 'stalled': 436,\n",
       " 'trapped': 437,\n",
       " 'useless': 438,\n",
       " 'waiting': 439,\n",
       " 'winners': 440,\n",
       " 'doll': 441,\n",
       " 'snob': 442,\n",
       " 'boring': 443,\n",
       " 'bright': 444,\n",
       " 'clever': 445,\n",
       " 'crafty': 446,\n",
       " 'creepy': 447,\n",
       " 'grumpy': 448,\n",
       " 'insane': 449,\n",
       " 'pretty': 450,\n",
       " 'stupid': 451,\n",
       " 'unfair': 452,\n",
       " 'canadian': 453,\n",
       " 'genius': 454,\n",
       " 'writer': 455,\n",
       " 'actor': 456,\n",
       " 'bankrupt': 457,\n",
       " 'uncle': 458,\n",
       " 'outgoing': 459,\n",
       " 'bit': 460,\n",
       " 'gambler': 461,\n",
       " 'slacker': 462,\n",
       " 'author': 463,\n",
       " 'ex': 464,\n",
       " 'con': 465,\n",
       " 'outlaw': 466,\n",
       " 'henpecked': 467,\n",
       " 'prison': 468,\n",
       " 'friend': 469,\n",
       " 'open': 470,\n",
       " 'hungarian': 471,\n",
       " 'boy': 472,\n",
       " 'tourist': 473,\n",
       " 'him': 474,\n",
       " 'london': 475,\n",
       " 'spot': 476,\n",
       " 'today': 477,\n",
       " 'guy': 478,\n",
       " 'musician': 479,\n",
       " 'real': 480,\n",
       " 'salesman': 481,\n",
       " 'against': 482,\n",
       " 'it': 483,\n",
       " 'thumbs': 484,\n",
       " 'astonished': 485,\n",
       " 'behind': 486,\n",
       " 'being': 487,\n",
       " 'used': 488,\n",
       " 'contagious': 489,\n",
       " 'dehydrated': 490,\n",
       " 'dependable': 491,\n",
       " 'devastated': 492,\n",
       " 'doing': 493,\n",
       " 'downstairs': 494,\n",
       " 'exercising': 495,\n",
       " 'farsighted': 496,\n",
       " 'fascinated': 497,\n",
       " 'firing': 498,\n",
       " 'from': 499,\n",
       " 'kyoto': 500,\n",
       " 'having': 501,\n",
       " 'illiterate': 502,\n",
       " 'car': 503,\n",
       " 'interested': 504,\n",
       " 'making': 505,\n",
       " 'tea': 506,\n",
       " 'meditating': 507,\n",
       " 'methodical': 508,\n",
       " 'quitter': 509,\n",
       " 'amused': 510,\n",
       " 'bitter': 511,\n",
       " 'guilty': 512,\n",
       " 'racist': 513,\n",
       " 'scared': 514,\n",
       " 'enough': 515,\n",
       " 'holiday': 516,\n",
       " 'one': 517,\n",
       " 'of': 518,\n",
       " 'optimistic': 519,\n",
       " 'gas': 520,\n",
       " 'prejudiced': 521,\n",
       " 'really': 522,\n",
       " 'reasonable': 523,\n",
       " 'remodeling': 524,\n",
       " 'still': 525,\n",
       " 'successful': 526,\n",
       " 'killer': 527,\n",
       " 'oldest': 528,\n",
       " 'undressing': 529,\n",
       " 'unemployed': 530,\n",
       " 'untalented': 531,\n",
       " 'to': 532,\n",
       " 'vegetarian': 533,\n",
       " 'wide': 534,\n",
       " 'nurse': 535,\n",
       " 'awkward': 536,\n",
       " 'isn': 537,\n",
       " 't': 538,\n",
       " 'beauty': 539,\n",
       " 'hottie': 540,\n",
       " 'looker': 541,\n",
       " 'angel': 542,\n",
       " 'pregnant': 543,\n",
       " 'loud': 544,\n",
       " 'babies': 545,\n",
       " 'idiots': 546,\n",
       " 'couple': 547,\n",
       " 'family': 548,\n",
       " 'brothers': 549,\n",
       " 'escaping': 550,\n",
       " 'grateful': 551,\n",
       " 'helpless': 552,\n",
       " 'obedient': 553,\n",
       " 'partners': 554,\n",
       " 'quitting': 555,\n",
       " 'retiring': 556,\n",
       " 'standing': 557,\n",
       " 'students': 558,\n",
       " 'best': 559,\n",
       " 'last': 560,\n",
       " 'same': 561,\n",
       " 'up': 562,\n",
       " 'morons': 563,\n",
       " 'aren': 564,\n",
       " 'prude': 565,\n",
       " 'amazing': 566,\n",
       " 'amusing': 567,\n",
       " 'callous': 568,\n",
       " 'elusive': 569,\n",
       " 'invited': 570,\n",
       " 'help': 571,\n",
       " 'precise': 572,\n",
       " 'pro': 573,\n",
       " 'welcome': 574,\n",
       " 'dreamer': 575,\n",
       " 'painter': 576,\n",
       " 'paid': 577,\n",
       " 'comedian': 578,\n",
       " 'frat': 579,\n",
       " 'freshman': 580,\n",
       " 'gardener': 581,\n",
       " 'newcomer': 582,\n",
       " 'slowpoke': 583,\n",
       " 'southpaw': 584,\n",
       " 'helping': 585,\n",
       " 'husband': 586,\n",
       " 'partner': 587,\n",
       " 'yet': 588,\n",
       " 'his': 589,\n",
       " 'photogenic': 590,\n",
       " 'bachelor': 591,\n",
       " 'egypt': 592,\n",
       " 'trouble': 593,\n",
       " 'azerbaijani': 594,\n",
       " 'tv': 595,\n",
       " 'addict': 596,\n",
       " 'tipsy': 597,\n",
       " 'foreigner': 598,\n",
       " 'housewife': 599,\n",
       " 'masochist': 600,\n",
       " 'night': 601,\n",
       " 'owl': 602,\n",
       " 'able': 603,\n",
       " 'run': 604,\n",
       " 'ski': 605,\n",
       " 'about': 606,\n",
       " 'adventurous': 607,\n",
       " 'engineer': 608,\n",
       " 'work': 609,\n",
       " 'begging': 610,\n",
       " 'catching': 611,\n",
       " 'celebrating': 612,\n",
       " 'cleaned': 613,\n",
       " 'color': 614,\n",
       " 'comfortable': 615,\n",
       " 'cooperating': 616,\n",
       " 'cracking': 617,\n",
       " 'embarrassed': 618,\n",
       " 'feeling': 619,\n",
       " 'low': 620,\n",
       " 'brazil': 621,\n",
       " 'france': 622,\n",
       " 'turkey': 623,\n",
       " 'zambia': 624,\n",
       " 'getting': 625,\n",
       " 'hardworking': 626,\n",
       " 'heartbroken': 627,\n",
       " 'win': 628,\n",
       " 'hurry': 629,\n",
       " 'interfering': 630,\n",
       " 'introverted': 631,\n",
       " 'left': 632,\n",
       " 'handed': 633,\n",
       " 'nearsighted': 634,\n",
       " 'child': 635,\n",
       " 'crook': 636,\n",
       " 'robot': 637,\n",
       " 'alarmed': 638,\n",
       " 'arguing': 639,\n",
       " 'nervous': 640,\n",
       " 'perfect': 641,\n",
       " 'unarmed': 642,\n",
       " 'vacation': 643,\n",
       " 'only': 644,\n",
       " 'ammo': 645,\n",
       " 'persevering': 646,\n",
       " 'quite': 647,\n",
       " 'rather': 648,\n",
       " 'go': 649,\n",
       " 'replaceable': 650,\n",
       " 'resourceful': 651,\n",
       " 'spontaneous': 652,\n",
       " 'such': 653,\n",
       " 'sympathetic': 654,\n",
       " 'captain': 655,\n",
       " 'surgeon': 656,\n",
       " 'truly': 657,\n",
       " 'trustworthy': 658,\n",
       " 'unambitious': 659,\n",
       " 'unconvinced': 660,\n",
       " 'modest': 661,\n",
       " 'watching': 662,\n",
       " 'lawyer': 663,\n",
       " 'sister': 664,\n",
       " 'singer': 665,\n",
       " 'typist': 666,\n",
       " 'graceful': 667,\n",
       " 'assertive': 668,\n",
       " 'roll': 669,\n",
       " 'actors': 670,\n",
       " 'melons': 671,\n",
       " 'animals': 672,\n",
       " 'doctors': 673,\n",
       " 'outside': 674,\n",
       " 'similar': 675,\n",
       " 'smiling': 676,\n",
       " 'with': 677,\n",
       " 'sons': 678,\n",
       " 'teachers': 679,\n",
       " 'canadians': 680,\n",
       " 'both': 681,\n",
       " 'comedians': 682,\n",
       " 'committed': 683,\n",
       " 'dedicated': 684,\n",
       " 'fixing': 685,\n",
       " 'flattered': 686,\n",
       " 'gardeners': 687,\n",
       " 'gentlemen': 688,\n",
       " 'giving': 689,\n",
       " 'charge': 690,\n",
       " 'newcomers': 691,\n",
       " 'newlyweds': 692,\n",
       " 'fools': 693,\n",
       " 'past': 694,\n",
       " 'that': 695,\n",
       " 'prisoners': 696,\n",
       " 'resigning': 697,\n",
       " 'separated': 698,\n",
       " 'survivors': 699,\n",
       " 'close': 700,\n",
       " 'unrelated': 701,\n",
       " 'naughty': 702,\n",
       " 'tallest': 703,\n",
       " 'idiot': 704,\n",
       " 'arrogant': 705,\n",
       " 'blushing': 706,\n",
       " 'careless': 707,\n",
       " 'charming': 708,\n",
       " 'cheating': 709,\n",
       " 'disloyal': 710,\n",
       " 'forgiven': 711,\n",
       " 'horrible': 712,\n",
       " 'mistaken': 713,\n",
       " 'enemy': 714,\n",
       " 'pathetic': 715,\n",
       " 'picky': 716,\n",
       " 'sweet': 717,\n",
       " 'stalling': 718,\n",
       " 'talented': 719,\n",
       " 'acrobat': 720,\n",
       " 'distracted': 721,\n",
       " 'her': 722,\n",
       " 'pajamas': 723,\n",
       " 'kid': 724,\n",
       " 'brother': 725,\n",
       " 'never': 726,\n",
       " 'liked': 727,\n",
       " 'bartender': 728,\n",
       " 'cat': 729,\n",
       " 'lover': 730,\n",
       " 'gentleman': 731,\n",
       " 'grown': 732,\n",
       " 'historian': 733,\n",
       " 'nonsmoker': 734,\n",
       " 'sophomore': 735,\n",
       " 'side': 736,\n",
       " 'got': 737,\n",
       " 'flu': 738,\n",
       " 'for': 739,\n",
       " 'class': 740,\n",
       " 'intelligent': 741,\n",
       " 'struck': 742,\n",
       " 'like': 743,\n",
       " 'us': 744,\n",
       " 'minded': 745,\n",
       " 'unconscious': 746,\n",
       " 'father': 747,\n",
       " 'professor': 748,\n",
       " 'volunteer': 749,\n",
       " 'cooking': 750,\n",
       " 'dumbfounded': 751,\n",
       " 'russia': 752,\n",
       " 'frying': 753,\n",
       " 'fish': 754,\n",
       " 'time': 755,\n",
       " 'weekly': 756,\n",
       " 'years': 757,\n",
       " 'cousin': 758,\n",
       " 'bank': 759,\n",
       " 'clerk': 760,\n",
       " 'shutterbug': 761,\n",
       " 'swim': 762,\n",
       " 'already': 763,\n",
       " 'always': 764,\n",
       " 'ambidextrous': 765,\n",
       " 'beach': 766,\n",
       " 'between': 767,\n",
       " 'jobs': 768,\n",
       " 'by': 769,\n",
       " 'disappointed': 770,\n",
       " 'dissatisfied': 771,\n",
       " 'lunch': 772,\n",
       " 'fairly': 773,\n",
       " 'freaking': 774,\n",
       " 'america': 775,\n",
       " 'croatia': 776,\n",
       " 'england': 777,\n",
       " 'romania': 778,\n",
       " 'homeschooled': 779,\n",
       " 'housesitting': 780,\n",
       " 'attic': 781,\n",
       " 'house': 782,\n",
       " 'inviting': 783,\n",
       " 'looking': 784,\n",
       " 'killing': 785,\n",
       " 'beggar': 786,\n",
       " 'bluffing': 787,\n",
       " 'bragging': 788,\n",
       " 'counting': 789,\n",
       " 'dreaming': 790,\n",
       " 'paranoid': 791,\n",
       " 'why': 792,\n",
       " 'ideas': 793,\n",
       " 'shape': 794,\n",
       " 'overreacting': 795,\n",
       " 'proud': 796,\n",
       " 'this': 797,\n",
       " 'stronger': 798,\n",
       " 'super': 799,\n",
       " 'volunteering': 800,\n",
       " 'pianist': 801,\n",
       " 'obstinate': 802,\n",
       " 'wise': 803,\n",
       " 'russian': 804,\n",
       " 'artists': 805,\n",
       " 'singers': 806,\n",
       " 'fake': 807,\n",
       " 'traitors': 808,\n",
       " 'unique': 809,\n",
       " 'classmates': 810,\n",
       " 'downsizing': 811,\n",
       " 'down': 812,\n",
       " 'east': 813,\n",
       " 'west': 814,\n",
       " 'half': 815,\n",
       " 'college': 816,\n",
       " 'control': 817,\n",
       " 'lifeguards': 818,\n",
       " 'needed': 819,\n",
       " 'our': 820,\n",
       " 'own': 821,\n",
       " 'speechless': 822,\n",
       " 'surrounded': 823,\n",
       " 'taking': 824,\n",
       " 'owners': 825,\n",
       " 'children': 826,\n",
       " 'deranged': 827,\n",
       " 'fabulous': 828,\n",
       " 'gorgeous': 829,\n",
       " 'hopeless': 830,\n",
       " 'blame': 831,\n",
       " 'traitor': 832,\n",
       " 'conceited': 833,\n",
       " 'courteous': 834,\n",
       " 'fortunate': 835,\n",
       " 'obnoxious': 836,\n",
       " 'shivering': 837,\n",
       " 'talkative': 838,\n",
       " 'tense': 839,\n",
       " 'unethical': 840,\n",
       " 'wonderful': 841,\n",
       " 'eater': 842,\n",
       " 'biologist': 843,\n",
       " 'born': 844,\n",
       " 'daredevil': 845,\n",
       " 'detective': 846,\n",
       " 'dramatist': 847,\n",
       " 'physicist': 848,\n",
       " 'scientist': 849,\n",
       " 'forty': 850,\n",
       " 'desk': 851,\n",
       " 'beyond': 852,\n",
       " 'hope': 853,\n",
       " 'business': 854,\n",
       " 'incompetent': 855,\n",
       " 'influential': 856,\n",
       " 'teaching': 857,\n",
       " 'unrealistic': 858,\n",
       " 'walking': 859,\n",
       " 'journalist': 860,\n",
       " 'movie': 861,\n",
       " 'buff': 862,\n",
       " 'timer': 863,\n",
       " 'undergrad': 864,\n",
       " 'deep': 865,\n",
       " 'debt': 866,\n",
       " 'georgia': 867,\n",
       " 'water': 868,\n",
       " 'arrived': 869,\n",
       " 'and': 870,\n",
       " 'raking': 871,\n",
       " 'quick': 872,\n",
       " 'sound': 873,\n",
       " 'swimming': 874,\n",
       " 'tickled': 875,\n",
       " 'pink': 876,\n",
       " 'trusting': 877,\n",
       " 'person': 878,\n",
       " 'almost': 879,\n",
       " 'bread': 880,\n",
       " 'math': 881,\n",
       " 'die': 882,\n",
       " 'pleased': 883,\n",
       " 'little': 884,\n",
       " 'girl': 885,\n",
       " 'salesperson': 886,\n",
       " 'total': 887,\n",
       " 'wreck': 888,\n",
       " 'myself': 889,\n",
       " 'as': 890,\n",
       " 'school': 891,\n",
       " 'mercy': 892,\n",
       " 'aware': 893,\n",
       " 'awfully': 894,\n",
       " 'watched': 895,\n",
       " 'concentrating': 896,\n",
       " 'extremely': 897,\n",
       " 'dizzy': 898,\n",
       " 'flabbergasted': 899,\n",
       " 'bulgaria': 900,\n",
       " 'colombia': 901,\n",
       " 'city': 902,\n",
       " 'did': 903,\n",
       " 'jump': 904,\n",
       " 'irreplaceable': 905,\n",
       " 'guessing': 906,\n",
       " 'weight': 907,\n",
       " 'monster': 908,\n",
       " 'soldier': 909,\n",
       " 'dangerous': 910,\n",
       " 'rush': 911,\n",
       " 'miserable': 912,\n",
       " 'panicking': 913,\n",
       " 'persuaded': 914,\n",
       " 'fast': 915,\n",
       " 'coach': 916,\n",
       " 'baby': 917,\n",
       " 'maid': 918,\n",
       " 'overworked': 919,\n",
       " 'sore': 920,\n",
       " 'sort': 921,\n",
       " 'dubious': 922,\n",
       " 'bath': 923,\n",
       " 'totally': 924,\n",
       " 'uncomfortable': 925,\n",
       " 'neighbor': 926,\n",
       " 'beginner': 927,\n",
       " 'aggressive': 928,\n",
       " 'attractive': 929,\n",
       " 'unsociable': 930,\n",
       " 'fashionable': 931,\n",
       " 'point': 932,\n",
       " 'hyperactive': 933,\n",
       " 'daughter': 934,\n",
       " 'alike': 935,\n",
       " 'liars': 936,\n",
       " 'cannibals': 937,\n",
       " 'expensive': 938,\n",
       " 'guys': 939,\n",
       " 'undamaged': 940,\n",
       " 'australian': 941,\n",
       " 'related': 942,\n",
       " 'risk': 943,\n",
       " 'cowards': 944,\n",
       " 'changing': 945,\n",
       " 'defenseless': 946,\n",
       " 'great': 947,\n",
       " 'experienced': 948,\n",
       " 'north': 949,\n",
       " 'south': 950,\n",
       " 'handling': 951,\n",
       " 'journalists': 952,\n",
       " 'dressed': 953,\n",
       " 'killers': 954,\n",
       " 'schedule': 955,\n",
       " 'beer': 956,\n",
       " 'milk': 957,\n",
       " 'wine': 958,\n",
       " 'pulling': 959,\n",
       " 'problem': 960,\n",
       " 'actresses': 961,\n",
       " 'hilarious': 962,\n",
       " 'prisoner': 963,\n",
       " 'articulate': 964,\n",
       " 'again': 965,\n",
       " 'courageous': 966,\n",
       " 'disgusting': 967,\n",
       " 'hurting': 968,\n",
       " 'incredible': 969,\n",
       " 'productive': 970,\n",
       " 'remarkable': 971,\n",
       " 'scaring': 972,\n",
       " 'telling': 973,\n",
       " 'leader': 974,\n",
       " 'master': 975,\n",
       " 'polite': 976,\n",
       " 'driver': 977,\n",
       " 'fishmonger': 978,\n",
       " 'loser': 979,\n",
       " 'slim': 980,\n",
       " 'but': 981,\n",
       " 'drinking': 982,\n",
       " 'classmate': 983,\n",
       " 'colleague': 984,\n",
       " 'radio': 985,\n",
       " 'playing': 986,\n",
       " 'golf': 987,\n",
       " 'witted': 988,\n",
       " 'heartless': 989,\n",
       " 'come': 990,\n",
       " 'thick': 991,\n",
       " 'headed': 992,\n",
       " 'learned': 993,\n",
       " 'creationist': 994,\n",
       " 'walker': 995,\n",
       " 'filthy': 996,\n",
       " 'food': 997,\n",
       " 'critic': 998,\n",
       " 'ghostwriter': 999,\n",
       " 'goal': 1000,\n",
       " 'keeper': 1001,\n",
       " ...}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 不同单词出现的次数\n",
    "output_lang.word2index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Seq2Seq Model\n",
    "\n",
    "### Seq2Seq的好处\n",
    "\n",
    "对比传统的单层的RNN来说, 可以不需要输入和输出是相同的长度的. \n",
    "\n",
    "下面是完整的思想, 这里的原文还是很不错的.\n",
    "\n",
    "Unlike sequence prediction with a single RNN, where every input corresponds to an output, the seq2seq model frees us from sequence length and order, which makes it ideal for translation between two languages.\n",
    "\n",
    "Consider the sentence “Je ne suis pas le chat noir” → “I am not the black cat”. Most of the words in the input sentence have a direct translation in the output sentence, but are in slightly different orders, e.g. “chat noir” and “black cat”. Because of the “ne/pas” construction there is also one more word in the input sentence. It would be difficult to produce a correct translation directly from the sequence of input words.\n",
    "\n",
    "With a seq2seq model the encoder creates a single vector which, in the ideal case, encodes the “meaning” of the input sequence into a single vector — a single point in some N dimensional space of sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Encoder\n",
    "\n",
    "The encoder of a seq2seq network is a RNN that outputs some value for every word from the input sentence. For every input word the encoder outputs a vector and a hidden state, and uses the hidden state for the next input word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.input_size, self.hidden_size)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.sentence_length = x.size(0)\n",
    "        embedded = self.embedding(x).view(self.sentence_length, 1, -1)\n",
    "        output = embedded\n",
    "        self.hidden = self.initHidden()\n",
    "        output, hidden = self.gru(output, self.hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 123],\n",
       "        [ 201],\n",
       "        [ 342],\n",
       "        [1515],\n",
       "        [   5],\n",
       "        [   1]], device='cuda:0')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = tensorsFromPair(random.choice(pairs))\n",
    "test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder测试\n",
    "encoder1 = EncoderRNN(input_lang.n_words, 256).to(device)\n",
    "output, hidden = encoder1(test_data[0].unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1, 256])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 256])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Decoder\n",
    "\n",
    "The decoder is another RNN that takes the encoder output vector(s) and outputs a sequence of words to create the translation.\n",
    "\n",
    "#### Simple Decoder\n",
    "\n",
    "In the simplest seq2seq decoder we use only last output of the encoder. This last output is sometimes called the context vector as it encodes context from the entire sequence. This context vector is used as the initial hidden state of the decoder.\n",
    "\n",
    "At every step of decoding, the decoder is given an input token and hidden state. The initial input token is the start-of-string <SOS> token, and the first hidden state is the context vector (the encoder’s last hidden state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x, hidden_state):\n",
    "        # 这里的hidden_state需要传入, 传入的是encoder最后输出的那个向量\n",
    "        self.hidden = hidden_state\n",
    "        \n",
    "        # 只能预测一个, 输入下一个\n",
    "        output = self.embedding(x).view(1,1,-1)\n",
    "        output = F.relu(output)\n",
    "        \n",
    "        output, hidden = self.gru(output, self.hidden)\n",
    "        output = self.out(output[0])\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0]], device='cuda:0')"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = torch.tensor([[SOS_token]]).to(device)\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_lang.n_words = 2803\n",
    "# output的单词数量有2803个\n",
    "decoder1 = DecoderRNN(256, output_lang.n_words).to(device)\n",
    "output, hidden = decoder1(test_data, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-7.4673]], device='cuda:0'), tensor([[2728]], device='cuda:0'))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 返回output最大值的位置\n",
    "output.data.topk(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2803])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这里有2803个单词, 每个单词的概率\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 256])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "### Preparing Training Data\n",
    "\n",
    "To train, for each pair we will need an input tensor (indexes of the words in the input sentence) and target tensor (indexes of the words in the target sentence). While creating these vectors we will append the EOS token to both sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  2],\n",
       "        [ 16],\n",
       "        [ 42],\n",
       "        [472],\n",
       "        [  1]], device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将一句话中的每个字母转为Index, 并在结尾加上终止符\n",
    "tensorFromSentence(output_lang, 'i am a boy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "\n",
    "To train we run the input sentence through the encoder, and keep track of every output and the latest hidden state. Then the decoder is given the <SOS> token as its first input, and the last hidden state of the encoder as its first hidden state.(总体训练流程)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    \"\"\"将秒转换为分钟\n",
    "    \"\"\"\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    \"\"\"打印已经花费的时间和预计花费的时间\n",
    "       预计花费的时间, 用 完成百分比的时间/现在完成的百分比 来预测\n",
    "    \"\"\"\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5 # 50%的概率使用teacher_forcing的模式\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    \n",
    "    # encoder_outputs = torch.zeros(max_length, encoder.hidden_size).to(device)\n",
    "    # Encoder\n",
    "    encoder_output, encoder_hidden = encoder1(input_tensor.unsqueeze(1))\n",
    "    \n",
    "    # Decoder\n",
    "    loss = 0\n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoder_input = torch.tensor([[SOS_token]]).to(device)\n",
    "    # 判断是使用哪一种模式\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    \n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            loss = loss + criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di] # Teacher Forcing\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach() # detach from history as input\n",
    "            loss = loss + criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "    # 反向传播, 进行优化\n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole training process looks like this:\n",
    "\n",
    "- Start a timer\n",
    "- Initialize optimizers and criterion\n",
    "- Create set of training pairs\n",
    "- Start empty losses array for plotting\n",
    "\n",
    "Then we call train many times and occasionally print the progress (% of examples, time so far, estimated time) and average loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0 # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "    \n",
    "    # 初始化优化器\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    # 初始化样本\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs)) for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "    \n",
    "    for iter in range(1, n_iters+1):\n",
    "        training_pair = training_pairs[iter-1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "        \n",
    "        loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total = print_loss_total + loss\n",
    "        plot_loss_total = plot_loss_total + loss\n",
    "        \n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ploting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure(figsize=(14,7))\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 42s (- 141m 16s) (500 0%) 3.6491\n",
      "0m 58s (- 96m 19s) (1000 1%) 3.2340\n",
      "1m 14s (- 81m 19s) (1500 1%) 3.1087\n",
      "1m 30s (- 74m 0s) (2000 2%) 2.9397\n",
      "1m 47s (- 69m 44s) (2500 2%) 2.8881\n",
      "2m 3s (- 66m 40s) (3000 3%) 2.6975\n",
      "2m 20s (- 64m 30s) (3500 3%) 2.7803\n",
      "2m 37s (- 62m 50s) (4000 4%) 2.6350\n",
      "2m 53s (- 61m 25s) (4500 4%) 2.5641\n",
      "3m 10s (- 60m 13s) (5000 5%) 2.6535\n",
      "3m 26s (- 59m 12s) (5500 5%) 2.4947\n",
      "3m 43s (- 58m 17s) (6000 6%) 2.5774\n",
      "3m 59s (- 57m 28s) (6500 6%) 2.3960\n",
      "4m 16s (- 56m 50s) (7000 7%) 2.4104\n",
      "4m 33s (- 56m 10s) (7500 7%) 2.4154\n",
      "4m 49s (- 55m 34s) (8000 8%) 2.2737\n",
      "5m 6s (- 55m 1s) (8500 8%) 2.2909\n",
      "5m 22s (- 54m 25s) (9000 9%) 2.2247\n",
      "5m 39s (- 53m 55s) (9500 9%) 2.2110\n",
      "5m 56s (- 53m 26s) (10000 10%) 2.1046\n",
      "6m 12s (- 52m 56s) (10500 10%) 2.1543\n",
      "6m 28s (- 52m 26s) (11000 11%) 2.1099\n",
      "6m 45s (- 52m 3s) (11500 11%) 2.1171\n",
      "7m 2s (- 51m 37s) (12000 12%) 2.0717\n",
      "7m 18s (- 51m 10s) (12500 12%) 2.0150\n",
      "7m 35s (- 50m 46s) (13000 13%) 2.0041\n",
      "7m 52s (- 50m 24s) (13500 13%) 1.9533\n",
      "8m 8s (- 50m 2s) (14000 14%) 1.9736\n",
      "8m 25s (- 49m 39s) (14500 14%) 1.9638\n",
      "8m 41s (- 49m 16s) (15000 15%) 1.8278\n",
      "8m 58s (- 48m 55s) (15500 15%) 1.8848\n",
      "9m 14s (- 48m 31s) (16000 16%) 1.8275\n",
      "9m 31s (- 48m 11s) (16500 16%) 1.8222\n",
      "9m 48s (- 47m 51s) (17000 17%) 1.8005\n",
      "10m 4s (- 47m 29s) (17500 17%) 1.7700\n",
      "10m 20s (- 47m 6s) (18000 18%) 1.8473\n",
      "10m 36s (- 46m 42s) (18500 18%) 1.6365\n",
      "10m 51s (- 46m 18s) (19000 19%) 1.7329\n",
      "11m 7s (- 45m 57s) (19500 19%) 1.7261\n",
      "11m 23s (- 45m 33s) (20000 20%) 1.7195\n",
      "11m 38s (- 45m 9s) (20500 20%) 1.7487\n",
      "11m 54s (- 44m 46s) (21000 21%) 1.6285\n",
      "12m 9s (- 44m 24s) (21500 21%) 1.6116\n",
      "12m 25s (- 44m 2s) (22000 22%) 1.5785\n",
      "12m 41s (- 43m 41s) (22500 22%) 1.5555\n",
      "12m 56s (- 43m 19s) (23000 23%) 1.5474\n",
      "13m 11s (- 42m 57s) (23500 23%) 1.5179\n",
      "13m 27s (- 42m 35s) (24000 24%) 1.5796\n",
      "13m 42s (- 42m 15s) (24500 24%) 1.4858\n",
      "13m 58s (- 41m 56s) (25000 25%) 1.3934\n",
      "14m 14s (- 41m 35s) (25500 25%) 1.4832\n",
      "14m 29s (- 41m 15s) (26000 26%) 1.4504\n",
      "14m 45s (- 40m 55s) (26500 26%) 1.4302\n",
      "15m 0s (- 40m 35s) (27000 27%) 1.3757\n",
      "15m 15s (- 40m 14s) (27500 27%) 1.4571\n",
      "15m 30s (- 39m 53s) (28000 28%) 1.3946\n",
      "15m 46s (- 39m 34s) (28500 28%) 1.4327\n",
      "16m 2s (- 39m 16s) (29000 28%) 1.4396\n",
      "16m 18s (- 38m 58s) (29500 29%) 1.3433\n",
      "16m 35s (- 38m 42s) (30000 30%) 1.3662\n",
      "16m 51s (- 38m 25s) (30500 30%) 1.3599\n",
      "17m 8s (- 38m 8s) (31000 31%) 1.1912\n",
      "17m 24s (- 37m 51s) (31500 31%) 1.2815\n",
      "17m 41s (- 37m 35s) (32000 32%) 1.2599\n",
      "17m 58s (- 37m 18s) (32500 32%) 1.2261\n",
      "18m 14s (- 37m 2s) (33000 33%) 1.1981\n",
      "18m 31s (- 36m 46s) (33500 33%) 1.2218\n",
      "18m 48s (- 36m 29s) (34000 34%) 1.1907\n",
      "19m 4s (- 36m 13s) (34500 34%) 1.2240\n",
      "19m 21s (- 35m 57s) (35000 35%) 1.0940\n",
      "19m 38s (- 35m 40s) (35500 35%) 1.1739\n",
      "19m 54s (- 35m 23s) (36000 36%) 1.1902\n",
      "20m 11s (- 35m 7s) (36500 36%) 1.1607\n",
      "20m 28s (- 34m 51s) (37000 37%) 1.2208\n",
      "20m 44s (- 34m 34s) (37500 37%) 1.0794\n",
      "21m 1s (- 34m 17s) (38000 38%) 1.0788\n",
      "21m 17s (- 34m 0s) (38500 38%) 1.1090\n",
      "21m 34s (- 33m 44s) (39000 39%) 1.0505\n",
      "21m 51s (- 33m 28s) (39500 39%) 1.0568\n",
      "22m 8s (- 33m 13s) (40000 40%) 1.0638\n",
      "22m 25s (- 32m 57s) (40500 40%) 1.0217\n",
      "22m 42s (- 32m 40s) (41000 41%) 0.9892\n",
      "22m 59s (- 32m 24s) (41500 41%) 1.0300\n",
      "23m 16s (- 32m 8s) (42000 42%) 0.9455\n",
      "23m 34s (- 31m 53s) (42500 42%) 1.0307\n",
      "23m 51s (- 31m 37s) (43000 43%) 1.0016\n",
      "24m 8s (- 31m 21s) (43500 43%) 0.9619\n",
      "24m 25s (- 31m 5s) (44000 44%) 0.9489\n",
      "24m 43s (- 30m 49s) (44500 44%) 0.9734\n",
      "25m 0s (- 30m 33s) (45000 45%) 0.9200\n",
      "25m 17s (- 30m 17s) (45500 45%) 0.9752\n",
      "25m 33s (- 30m 0s) (46000 46%) 0.9571\n",
      "25m 50s (- 29m 44s) (46500 46%) 0.9013\n",
      "26m 7s (- 29m 27s) (47000 47%) 0.8623\n",
      "26m 24s (- 29m 10s) (47500 47%) 0.8723\n",
      "26m 40s (- 28m 53s) (48000 48%) 0.9039\n",
      "26m 56s (- 28m 36s) (48500 48%) 0.8282\n",
      "27m 13s (- 28m 19s) (49000 49%) 0.8628\n",
      "27m 29s (- 28m 2s) (49500 49%) 0.9166\n",
      "27m 45s (- 27m 45s) (50000 50%) 0.8650\n",
      "28m 1s (- 27m 28s) (50500 50%) 0.7906\n",
      "28m 18s (- 27m 11s) (51000 51%) 0.9809\n",
      "28m 34s (- 26m 54s) (51500 51%) 0.8129\n",
      "28m 51s (- 26m 37s) (52000 52%) 0.8284\n",
      "29m 7s (- 26m 21s) (52500 52%) 0.7464\n",
      "29m 23s (- 26m 4s) (53000 53%) 0.7260\n",
      "29m 39s (- 25m 47s) (53500 53%) 0.7695\n",
      "29m 57s (- 25m 30s) (54000 54%) 0.7775\n",
      "30m 13s (- 25m 13s) (54500 54%) 0.7912\n",
      "30m 29s (- 24m 56s) (55000 55%) 0.7464\n",
      "30m 45s (- 24m 39s) (55500 55%) 0.6634\n",
      "31m 0s (- 24m 22s) (56000 56%) 0.7350\n",
      "31m 17s (- 24m 5s) (56500 56%) 0.7610\n",
      "31m 34s (- 23m 49s) (57000 56%) 0.7740\n",
      "31m 51s (- 23m 33s) (57500 57%) 0.6832\n",
      "32m 8s (- 23m 16s) (58000 57%) 0.7024\n",
      "32m 24s (- 22m 59s) (58500 58%) 0.6951\n",
      "32m 40s (- 22m 42s) (59000 59%) 0.7071\n",
      "32m 56s (- 22m 25s) (59500 59%) 0.6757\n",
      "33m 12s (- 22m 8s) (60000 60%) 0.7496\n",
      "33m 27s (- 21m 50s) (60500 60%) 0.6660\n",
      "33m 43s (- 21m 33s) (61000 61%) 0.7061\n",
      "33m 59s (- 21m 16s) (61500 61%) 0.7064\n",
      "34m 15s (- 20m 59s) (62000 62%) 0.6297\n",
      "34m 31s (- 20m 42s) (62500 62%) 0.6865\n",
      "34m 47s (- 20m 25s) (63000 63%) 0.6495\n",
      "35m 3s (- 20m 8s) (63500 63%) 0.6273\n",
      "35m 19s (- 19m 51s) (64000 64%) 0.6295\n",
      "35m 35s (- 19m 35s) (64500 64%) 0.5982\n",
      "35m 51s (- 19m 18s) (65000 65%) 0.6202\n",
      "36m 7s (- 19m 1s) (65500 65%) 0.6460\n",
      "36m 23s (- 18m 44s) (66000 66%) 0.6058\n",
      "36m 39s (- 18m 28s) (66500 66%) 0.5556\n",
      "36m 55s (- 18m 11s) (67000 67%) 0.5714\n",
      "37m 11s (- 17m 54s) (67500 67%) 0.5607\n",
      "37m 27s (- 17m 37s) (68000 68%) 0.5636\n",
      "37m 43s (- 17m 20s) (68500 68%) 0.5790\n",
      "37m 58s (- 17m 3s) (69000 69%) 0.5778\n",
      "38m 14s (- 16m 46s) (69500 69%) 0.5570\n",
      "38m 31s (- 16m 30s) (70000 70%) 0.6213\n",
      "38m 48s (- 16m 14s) (70500 70%) 0.5557\n",
      "39m 5s (- 15m 57s) (71000 71%) 0.5483\n",
      "39m 22s (- 15m 41s) (71500 71%) 0.5535\n",
      "39m 38s (- 15m 25s) (72000 72%) 0.4665\n",
      "39m 55s (- 15m 8s) (72500 72%) 0.5134\n",
      "40m 12s (- 14m 52s) (73000 73%) 0.5153\n",
      "40m 29s (- 14m 35s) (73500 73%) 0.4807\n",
      "40m 45s (- 14m 19s) (74000 74%) 0.5229\n",
      "41m 2s (- 14m 2s) (74500 74%) 0.4954\n",
      "41m 18s (- 13m 46s) (75000 75%) 0.4770\n",
      "41m 35s (- 13m 29s) (75500 75%) 0.4513\n",
      "41m 52s (- 13m 13s) (76000 76%) 0.4618\n",
      "42m 8s (- 12m 56s) (76500 76%) 0.5060\n",
      "42m 25s (- 12m 40s) (77000 77%) 0.4772\n",
      "42m 41s (- 12m 23s) (77500 77%) 0.5129\n",
      "42m 56s (- 12m 6s) (78000 78%) 0.4389\n",
      "43m 11s (- 11m 49s) (78500 78%) 0.3919\n",
      "43m 27s (- 11m 33s) (79000 79%) 0.4107\n",
      "43m 43s (- 11m 16s) (79500 79%) 0.4401\n",
      "43m 59s (- 10m 59s) (80000 80%) 0.4929\n",
      "44m 15s (- 10m 43s) (80500 80%) 0.3800\n",
      "44m 31s (- 10m 26s) (81000 81%) 0.3987\n",
      "44m 48s (- 10m 10s) (81500 81%) 0.4344\n",
      "45m 4s (- 9m 53s) (82000 82%) 0.4196\n",
      "45m 20s (- 9m 37s) (82500 82%) 0.4130\n",
      "45m 37s (- 9m 20s) (83000 83%) 0.4094\n",
      "45m 54s (- 9m 4s) (83500 83%) 0.4141\n",
      "46m 11s (- 8m 47s) (84000 84%) 0.4125\n",
      "46m 27s (- 8m 31s) (84500 84%) 0.3434\n",
      "46m 44s (- 8m 14s) (85000 85%) 0.3363\n",
      "47m 0s (- 7m 58s) (85500 85%) 0.3836\n",
      "47m 17s (- 7m 41s) (86000 86%) 0.4134\n",
      "47m 34s (- 7m 25s) (86500 86%) 0.3830\n",
      "47m 51s (- 7m 9s) (87000 87%) 0.3803\n",
      "48m 8s (- 6m 52s) (87500 87%) 0.3980\n",
      "48m 25s (- 6m 36s) (88000 88%) 0.3761\n",
      "48m 42s (- 6m 19s) (88500 88%) 0.3853\n",
      "48m 58s (- 6m 3s) (89000 89%) 0.4207\n",
      "49m 15s (- 5m 46s) (89500 89%) 0.3262\n",
      "49m 31s (- 5m 30s) (90000 90%) 0.3370\n",
      "49m 48s (- 5m 13s) (90500 90%) 0.3470\n",
      "50m 5s (- 4m 57s) (91000 91%) 0.3966\n",
      "50m 22s (- 4m 40s) (91500 91%) 0.3934\n",
      "50m 38s (- 4m 24s) (92000 92%) 0.3763\n",
      "50m 55s (- 4m 7s) (92500 92%) 0.3379\n",
      "51m 11s (- 3m 51s) (93000 93%) 0.2930\n",
      "51m 27s (- 3m 34s) (93500 93%) 0.2963\n",
      "51m 43s (- 3m 18s) (94000 94%) 0.3383\n",
      "51m 59s (- 3m 1s) (94500 94%) 0.3288\n",
      "52m 15s (- 2m 45s) (95000 95%) 0.3418\n",
      "52m 31s (- 2m 28s) (95500 95%) 0.3078\n",
      "52m 46s (- 2m 11s) (96000 96%) 0.3112\n",
      "53m 1s (- 1m 55s) (96500 96%) 0.2702\n",
      "53m 17s (- 1m 38s) (97000 97%) 0.3361\n",
      "53m 33s (- 1m 22s) (97500 97%) 0.2767\n",
      "53m 50s (- 1m 5s) (98000 98%) 0.2916\n",
      "54m 7s (- 0m 49s) (98500 98%) 0.3239\n",
      "54m 24s (- 0m 32s) (99000 99%) 0.2680\n",
      "54m 41s (- 0m 16s) (99500 99%) 0.2726\n",
      "54m 58s (- 0m 0s) (100000 100%) 0.2813\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1008x504 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "decoder1 = DecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
    "\n",
    "trainIters(encoder1, decoder1, n_iters=100000, print_every=500, plot_every=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Evaluation is mostly the same as training, but there are no targets so we simply feed the decoder’s predictions back to itself for each step. Every time it predicts a word we add it to the output string, and if it predicts the EOS token we stop there. We also store the decoder’s attention outputs for display later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        \n",
    "        # Encoder\n",
    "        encoder_output, encoder_hidden = encoder1(input_tensor.unsqueeze(1))\n",
    "        \n",
    "        # Decoder\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoded_words = []\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden  )\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                \"\"\"遇到终止符就停止\n",
    "                \"\"\"\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                \"\"\"把decode的word加入数组中\n",
    "                \"\"\"\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "                \n",
    "            # 下一个的输入是上一个的输出\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> je suis egalement heureuse .\n",
      "= i m happy too .\n",
      "< i m happy too . <EOS>\n",
      "\n",
      "> j attends de toi un travail serieux .\n",
      "= i am expecting some serious work from you .\n",
      "< i am expecting expecting serious serious serious . <EOS>\n",
      "\n",
      "> tu es chanceuse .\n",
      "= you re fortunate .\n",
      "< you re fortunate . <EOS>\n",
      "\n",
      "> je suis aussi choque que toi .\n",
      "= i m as shocked as you are .\n",
      "< i m as shocked as you are . <EOS>\n",
      "\n",
      "> elles sont armees .\n",
      "= they re armed .\n",
      "< they re armed . <EOS>\n",
      "\n",
      "> il est toujours chez lui le lundi .\n",
      "= he is always at home on mondays .\n",
      "< he s always at home on mondays . <EOS>\n",
      "\n",
      "> tu es distraite .\n",
      "= you re forgetful .\n",
      "< you re forgetful . <EOS>\n",
      "\n",
      "> nous sommes timides .\n",
      "= we re shy .\n",
      "< we re shy . <EOS>\n",
      "\n",
      "> il est affaire en dehors de la ville .\n",
      "= he s out of town on business .\n",
      "< he s out of town on business . <EOS>\n",
      "\n",
      "> je suis content de vous revoir .\n",
      "= i m glad to see you back .\n",
      "< i m glad to see you again . <EOS>\n",
      "\n",
      "> vous etes surmenes .\n",
      "= you re overworked .\n",
      "< you re overworked . <EOS>\n",
      "\n",
      "> je lance un signal de detresse .\n",
      "= i m declaring an emergency .\n",
      "< i m declaring an easter . <EOS>\n",
      "\n",
      "> tu n es pas grosse .\n",
      "= you re not fat .\n",
      "< you re not fat . <EOS>\n",
      "\n",
      "> je suis meilleur que toi .\n",
      "= i am better than you .\n",
      "< i am better than you . <EOS>\n",
      "\n",
      "> tu n es pas different .\n",
      "= you re no different .\n",
      "< you re no different . <EOS>\n",
      "\n",
      "> nous ne sommes pas vieux .\n",
      "= we re not old .\n",
      "< we re not old . <EOS>\n",
      "\n",
      "> nous sommes seules .\n",
      "= we re on our own .\n",
      "< we are alone . <EOS>\n",
      "\n",
      "> tu es un etudiant .\n",
      "= you are a student .\n",
      "< you are a student . <EOS>\n",
      "\n",
      "> ils sont en danger .\n",
      "= they re in danger .\n",
      "< they re in danger . <EOS>\n",
      "\n",
      "> tu sais te faire comprendre .\n",
      "= you re assertive .\n",
      "< you re assertive . <EOS>\n",
      "\n",
      "> je vous ecoute .\n",
      "= i m listening to you .\n",
      "< i m listening to you . <EOS>\n",
      "\n",
      "> elle est bienveillante .\n",
      "= she is good natured .\n",
      "< she is good natured . <EOS>\n",
      "\n",
      "> nous sommes deja en retard .\n",
      "= we re already late .\n",
      "< we re already late . <EOS>\n",
      "\n",
      "> les jeans sont en rupture de stock .\n",
      "= we are sold out of jeans .\n",
      "< we are sold out of jeans . <EOS>\n",
      "\n",
      "> il est en prison .\n",
      "= he s in prison .\n",
      "< he s in prison . <EOS>\n",
      "\n",
      "> je vous en supplie .\n",
      "= i m begging you .\n",
      "< i m begging you . <EOS>\n",
      "\n",
      "> elles ont toutes disparu .\n",
      "= they re all gone .\n",
      "< they re all gone . <EOS>\n",
      "\n",
      "> elle est trop agee pour toi .\n",
      "= she s too old for you .\n",
      "< she s too old for you . <EOS>\n",
      "\n",
      "> je n y suis pas tres bon .\n",
      "= i m not very good at it .\n",
      "< i m not very good . <EOS>\n",
      "\n",
      "> tu es un etudiant .\n",
      "= you are a student .\n",
      "< you are a student . <EOS>\n",
      "\n",
      "> nous sommes impitoyables .\n",
      "= we re ruthless .\n",
      "< we re ruthless . <EOS>\n",
      "\n",
      "> vous allez rire .\n",
      "= you re going to laugh .\n",
      "< you re going to laugh . <EOS>\n",
      "\n",
      "> je suis toujours inquiet .\n",
      "= i m still concerned .\n",
      "< i m still concerned . <EOS>\n",
      "\n",
      "> je suis aneanti .\n",
      "= i m devastated .\n",
      "< i m devastated . <EOS>\n",
      "\n",
      "> je suis simplement fatiguee .\n",
      "= i m just tired .\n",
      "< i m just tired . <EOS>\n",
      "\n",
      "> je suis tellement gene que je veux mourir .\n",
      "= i m so embarrassed i want to die .\n",
      "< i m so embarrassed i want to die . <EOS>\n",
      "\n",
      "> il a honte de son corps .\n",
      "= he s ashamed of his body .\n",
      "< he s ashamed of his body . <EOS>\n",
      "\n",
      "> je mange .\n",
      "= i m eating .\n",
      "< i m eating . <EOS>\n",
      "\n",
      "> je suis creve .\n",
      "= i am tired .\n",
      "< i m exhausted . <EOS>\n",
      "\n",
      "> il n est pas chez lui .\n",
      "= he s not at home .\n",
      "< he s not at home . <EOS>\n",
      "\n",
      "> c est un renard .\n",
      "= she is a fox .\n",
      "< she s a fox . <EOS>\n",
      "\n",
      "> ils sont tous decedes .\n",
      "= they re all dead .\n",
      "< they re all dead . <EOS>\n",
      "\n",
      "> il est cardiologue .\n",
      "= he s a cardiologist .\n",
      "< he s a cardiologist . <EOS>\n",
      "\n",
      "> je me devets .\n",
      "= i am undressing .\n",
      "< i am undressing . <EOS>\n",
      "\n",
      "> tu n es pas trop en retard .\n",
      "= you re not too late .\n",
      "< you re not too late . <EOS>\n",
      "\n",
      "> vous n etes pas tres amusants .\n",
      "= you re not very funny .\n",
      "< you re not very funny . <EOS>\n",
      "\n",
      "> pour l instant je m entraine seulement .\n",
      "= i am just warming up now .\n",
      "< i am only warming up now . <EOS>\n",
      "\n",
      "> elle a dix huit ans au plus .\n",
      "= she is eighteen at most .\n",
      "< she is eighteen at most . <EOS>\n",
      "\n",
      "> nous ne sommes pas en securite .\n",
      "= we re not safe .\n",
      "< we re not safe . <EOS>\n",
      "\n",
      "> nous sommes les meilleurs amis .\n",
      "= we re best friends .\n",
      "< we re best friends . <EOS>\n",
      "\n",
      "> je me rejouis de vous revoir .\n",
      "= i am looking forward to seeing you again .\n",
      "< i m looking forward to seeing you again . <EOS>\n",
      "\n",
      "> tu n es pas malade .\n",
      "= you re not sick .\n",
      "< you re not sick . <EOS>\n",
      "\n",
      "> t es un bon garcon !\n",
      "= you are a good boy .\n",
      "< you are a good boy . <EOS>\n",
      "\n",
      "> c est un gentleman .\n",
      "= he s a gentleman .\n",
      "< he s a gentleman . <EOS>\n",
      "\n",
      "> elle nous enseigne le francais .\n",
      "= she is teaching us french .\n",
      "< she is teaching us french . <EOS>\n",
      "\n",
      "> ca va pas le faire .\n",
      "= we re not gonna make it .\n",
      "< we re not gonna make . <EOS>\n",
      "\n",
      "> j ai soif .\n",
      "= i am thirsty .\n",
      "< i am thirsty . <EOS>\n",
      "\n",
      "> vous etes extraverties .\n",
      "= you re extroverted .\n",
      "< you re extroverted . <EOS>\n",
      "\n",
      "> il est rarement de bonne humeur .\n",
      "= he is rarely in a good mood .\n",
      "< he is rarely in a bad mood . <EOS>\n",
      "\n",
      "> vous etes en grand danger .\n",
      "= you re in grave danger .\n",
      "< you re in danger danger . <EOS>\n",
      "\n",
      "> vous etes trop tendus .\n",
      "= you re too tense .\n",
      "< you re too tense . <EOS>\n",
      "\n",
      "> tu es un sacre menteur .\n",
      "= you re such a liar .\n",
      "< you re such a liar . <EOS>\n",
      "\n",
      "> je ne suis pas encore prete .\n",
      "= i m not ready yet .\n",
      "< i m not ready yet . <EOS>\n",
      "\n",
      "> je ne suis pas tres occupe .\n",
      "= i m not very busy .\n",
      "< i m not very busy . <EOS>\n",
      "\n",
      "> je me brosse les dents .\n",
      "= i am brushing my teeth .\n",
      "< i am brushing my teeth . <EOS>\n",
      "\n",
      "> ils ont de la chance d etre vivants .\n",
      "= they re lucky to be alive .\n",
      "< they re lucky to be alive . <EOS>\n",
      "\n",
      "> nous sommes desormais en securite .\n",
      "= we re safe now .\n",
      "< we re safe now . <EOS>\n",
      "\n",
      "> je suis heureux de t avoir invite .\n",
      "= i m glad i invited you .\n",
      "< i m glad i invited you . <EOS>\n",
      "\n",
      "> vous etes un bon client .\n",
      "= you are a good customer .\n",
      "< you are a good customer . <EOS>\n",
      "\n",
      "> je ne suis pas interesse par votre opinion .\n",
      "= i m not interested in your opinion .\n",
      "< i m not interested in your opinion . <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1, decoder1, n=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
