{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入数据\n",
    "\n",
    "- 导入数据\n",
    "- 并将数据保存为pkl的格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_pickle(item, directory):\n",
    "    \"\"\"导出数据\n",
    "    \"\"\"\n",
    "    pickle.dump(item, open(directory,\"wb\"))\n",
    "\n",
    "\n",
    "def load_from_pickle(directory):\n",
    "    \"\"\"导入数据\n",
    "    \"\"\"\n",
    "    return pickle.load(open(directory,\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/train.csv',lineterminator='\\n',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Jo bhi ap se tou behtar hoon</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>ya Allah meri sister Affia ki madad farma</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Yeh khud chahta a is umar main shadi krna.  ha...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Tc ? Apky mun xe exe alfax achy nae lgty 😒💃</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Good</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                             review     label\n",
       "0   1                       Jo bhi ap se tou behtar hoon  Negative\n",
       "1   2          ya Allah meri sister Affia ki madad farma  Positive\n",
       "2   3  Yeh khud chahta a is umar main shadi krna.  ha...  Negative\n",
       "3   4        Tc ? Apky mun xe exe alfax achy nae lgty 😒💃  Negative\n",
       "4   5                                               Good  Positive"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据的预处理\n",
    "\n",
    "数据预处理大致分为下面几个部分.\n",
    "\n",
    "1. 将所有字母转为Ascii\n",
    "2. 将大写都转换为小写; 同时, 只保留常用的标点符号\n",
    "3. 新建完成, word2idx, idx2word, word2count(每个单词出现的次数), n_word(总的单词个数)\n",
    "4. 将句子转换为Tensor, 每个word使用index来进行代替\n",
    "5. 对句子进行填充, 使每句句子的长度相同, 这样可以使用batch进行训练\n",
    "6. 将label转换为one-hot的格式, 方便最后的训练(Pytorch中只需要转换为标号即可)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建word2index和index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一步数据预处理\n",
    "def unicodeToAscii(s):\n",
    "    \"\"\"转换为Ascii\n",
    "    \"\"\"\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# 第二步数据预处理\n",
    "def normalizeString(s):\n",
    "    \"\"\"转换为小写, 同时去掉奇怪的符号\n",
    "    \"\"\"\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "# 第三步数据预处理\n",
    "class Lang():\n",
    "    def __init__(self):\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        # 初始的时候SOS表示句子开头(0还在padding的时候会进行填充), EOS表示句子结尾(或是表示没有加入index中的新的单词, 即不常用的单词)\n",
    "        self.index2word = {0:\"SOS\",1:\"EOS\"}\n",
    "        self.n_words = 2\n",
    "    \n",
    "    def addSentence(self, sentence):\n",
    "        \"\"\"把句子中的每个单词加入字典中\n",
    "        \"\"\"\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "            \n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words # 新单词的标号\n",
    "            self.word2count[word] = 1 # 新单词的个数\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words = self.n_words + 1\n",
    "        else:\n",
    "            self.word2count[word] = self.word2count[word] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count word:17684\n"
     ]
    }
   ],
   "source": [
    "# 把数据集中的每句话中的单词进行转化\n",
    "lang = Lang()\n",
    "for sentence_data in data[\"review\"].values.tolist():\n",
    "    # 数据清洗\n",
    "    sentence_data = normalizeString(sentence_data)\n",
    "    # 增加word2index\n",
    "    lang.addSentence(sentence_data)\n",
    "\n",
    "# 显示一些统计数据\n",
    "print(\"Count word:{}\".format(lang.n_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 5.799343965614749, 3033)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 打印一下单词个数的分布\n",
    "data_count = np.array(list(lang.word2count.values()))\n",
    "\n",
    "# 有大量单词只出现了很少的次数\n",
    "np.median(data_count), np.mean(data_count), np.max(data_count) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "小于N的单词出现次数 :  10547 \n",
      "总的单词出现次数 :  102544 \n",
      "小于N的单词占比 :  10.285340926821657\n"
     ]
    }
   ],
   "source": [
    "# 计算<n的单词, 出现次数占总的出现次数的比例\n",
    "less_count = 0\n",
    "total_count = 0\n",
    "\n",
    "for _,count in lang.word2count.items():\n",
    "    if count < 2:\n",
    "        less_count = less_count + count\n",
    "    total_count = total_count + count\n",
    "\n",
    "print(\"小于N的单词出现次数 : \",less_count,\n",
    "      \"\\n总的单词出现次数 : \",total_count,\n",
    "      \"\\n小于N的单词占比 : \",less_count/total_count*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count word:7137\n"
     ]
    }
   ],
   "source": [
    "# 我们设置单词至少出现2次\n",
    "lang_process = Lang()\n",
    "\n",
    "for word,count in lang.word2count.items():\n",
    "    if count >= 2:\n",
    "        lang_process.word2index[word] = lang_process.n_words # 新单词的标号\n",
    "        lang_process.word2count[word] = count # 新单词的个数\n",
    "        lang_process.index2word[lang_process.n_words] = word\n",
    "        lang_process.n_words = lang_process.n_words + 1\n",
    "        \n",
    "# 显示一些统计数据\n",
    "print(\"Count word:{}\".format(lang_process.n_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'jo': 319,\n",
       " 'bhi': 670,\n",
       " 'ap': 276,\n",
       " 'se': 1235,\n",
       " 'tou': 99,\n",
       " 'behtar': 43,\n",
       " 'hoon': 44,\n",
       " 'ya': 182,\n",
       " 'allah': 508,\n",
       " 'meri': 129,\n",
       " 'sister': 4,\n",
       " 'ki': 2298,\n",
       " 'madad': 39,\n",
       " 'farma': 31,\n",
       " 'yeh': 231,\n",
       " 'khud': 96,\n",
       " 'chahta': 11,\n",
       " 'a': 90,\n",
       " 'is': 597,\n",
       " 'umar': 38,\n",
       " 'main': 335,\n",
       " 'shadi': 63,\n",
       " 'krna': 38,\n",
       " '.': 3033,\n",
       " 'had': 26,\n",
       " 'tc': 2,\n",
       " '?': 349,\n",
       " 'apky': 6,\n",
       " 'mun': 6,\n",
       " 'xe': 3,\n",
       " 'exe': 2,\n",
       " 'achy': 15,\n",
       " 'nae': 48,\n",
       " 'lgty': 3,\n",
       " '': 642,\n",
       " 'good': 110,\n",
       " 'american': 12,\n",
       " 'president': 8,\n",
       " 'john': 2,\n",
       " 'f': 6,\n",
       " 'kennedy': 2,\n",
       " 'aur': 1264,\n",
       " 'in': 368,\n",
       " 'ke': 2002,\n",
       " 'bhai': 124,\n",
       " 'marilyn': 8,\n",
       " 'monroe': 5,\n",
       " 'ka': 1380,\n",
       " 'charcha': 3,\n",
       " 'raha': 138,\n",
       " 'commission': 6,\n",
       " 'dor': 27,\n",
       " 'dora': 4,\n",
       " 'quomi': 5,\n",
       " 'farokhat': 2,\n",
       " 'keye': 11,\n",
       " 'gaye': 96,\n",
       " 'pak': 138,\n",
       " 'nazer': 6,\n",
       " 'e': 427,\n",
       " 'bd': 3,\n",
       " 'sy': 129,\n",
       " 'bechye': 2,\n",
       " 'or': 420,\n",
       " 'humesha': 2,\n",
       " 'bohat': 177,\n",
       " 'dy': 67,\n",
       " 'ameeen': 3,\n",
       " 'amoman': 2,\n",
       " 'log': 95,\n",
       " 'samajhte': 5,\n",
       " 'hain': 608,\n",
       " 'jhok': 2,\n",
       " 'hi': 330,\n",
       " 'abid': 8,\n",
       " 'ali': 82,\n",
       " 'pheli': 13,\n",
       " 'serial': 8,\n",
       " 'thi': 214,\n",
       " 'halan': 3,\n",
       " 'sach': 14,\n",
       " 'ye': 424,\n",
       " 'hai': 1562,\n",
       " 'unhon': 79,\n",
       " 'ne': 795,\n",
       " 'sab': 158,\n",
       " 'phele': 17,\n",
       " 'saleem': 3,\n",
       " 'le': 61,\n",
       " 'likhe': 8,\n",
       " 'howe': 11,\n",
       " 'kheel': 11,\n",
       " 'mein': 1536,\n",
       " 'adakari': 8,\n",
       " 'k': 1126,\n",
       " 'tum': 103,\n",
       " 'v': 23,\n",
       " 'wesy': 8,\n",
       " 'billi': 2,\n",
       " 'ho': 655,\n",
       " 'jail': 25,\n",
       " 'road': 23,\n",
       " 'pr': 104,\n",
       " 'firing': 39,\n",
       " 'shaks': 20,\n",
       " 'janbahaq': 26,\n",
       " 'rpt': 4,\n",
       " 'm': 74,\n",
       " 'jokhio': 4,\n",
       " 'hud': 3,\n",
       " 'stupid': 2,\n",
       " 'actor': 12,\n",
       " 'hy': 272,\n",
       " 'ati': 18,\n",
       " 'he': 326,\n",
       " 'nahi': 424,\n",
       " 'isay': 9,\n",
       " 'kiya': 271,\n",
       " 'dakh': 5,\n",
       " 'liya': 59,\n",
       " 'app': 63,\n",
       " 'nay': 120,\n",
       " 'shaadi': 2,\n",
       " 'kr': 189,\n",
       " 'loo': 4,\n",
       " 'yar': 43,\n",
       " 'haha': 19,\n",
       " 'thank': 27,\n",
       " 'you': 69,\n",
       " 'so': 33,\n",
       " 'much': 11,\n",
       " 'pakistani': 58,\n",
       " 'cricket': 50,\n",
       " 'tareekh': 21,\n",
       " 'behtreen': 8,\n",
       " 'batsman': 11,\n",
       " 'all': 38,\n",
       " 'time': 42,\n",
       " 'grades': 2,\n",
       " 'kia': 188,\n",
       " 'numaya': 4,\n",
       " 'muqam': 10,\n",
       " 'pakistan': 270,\n",
       " 'ko': 1493,\n",
       " 'naya': 19,\n",
       " 'chehra': 2,\n",
       " 'dene': 20,\n",
       " 'wale': 82,\n",
       " 'javed': 17,\n",
       " 'miandad': 21,\n",
       " 'june': 13,\n",
       " 'karachi': 67,\n",
       " 'paida': 30,\n",
       " 'huwe': 11,\n",
       " 'nawaz': 65,\n",
       " 'sharif': 51,\n",
       " 'pakro': 2,\n",
       " 'police': 67,\n",
       " 'walo': 30,\n",
       " 'logon': 68,\n",
       " 'again': 3,\n",
       " 'maa': 27,\n",
       " 'me': 389,\n",
       " 'land': 6,\n",
       " 'kash': 10,\n",
       " 'maan': 6,\n",
       " 'zinda': 19,\n",
       " 'hoti': 70,\n",
       " 'billion': 4,\n",
       " 'dollor': 2,\n",
       " 'zaid': 10,\n",
       " 'akhrajat': 5,\n",
       " 'takhmina': 2,\n",
       " 'lagaya': 13,\n",
       " 'kisi': 130,\n",
       " 'larki': 12,\n",
       " 'kay': 271,\n",
       " 'sat': 11,\n",
       " 'selfie': 3,\n",
       " 'lana': 7,\n",
       " 'hay': 75,\n",
       " 'maut': 9,\n",
       " 'yad': 40,\n",
       " 'dilata': 3,\n",
       " 'namaz': 18,\n",
       " 'hamari': 12,\n",
       " 'izafa': 7,\n",
       " 'karti': 29,\n",
       " 'hein': 36,\n",
       " 'wasy': 3,\n",
       " 'ma': 107,\n",
       " 'sporting': 3,\n",
       " 'lahore': 66,\n",
       " 'team': 64,\n",
       " 'tha': 376,\n",
       " 'lakin': 52,\n",
       " 'admin': 10,\n",
       " 'sb': 66,\n",
       " 'py': 42,\n",
       " 'lanat': 72,\n",
       " 'ni': 122,\n",
       " 'krni': 10,\n",
       " 'cahiya': 5,\n",
       " 'rakhay': 12,\n",
       " 'ga': 186,\n",
       " 'hum': 155,\n",
       " 'milkar': 2,\n",
       " 'islami': 11,\n",
       " 'hifazat': 18,\n",
       " 'karni': 8,\n",
       " 'hony': 17,\n",
       " 'farz': 5,\n",
       " 'ada': 24,\n",
       " 'krein': 5,\n",
       " 'dar': 19,\n",
       " 'asal': 9,\n",
       " 'parents': 12,\n",
       " 'ganey': 2,\n",
       " 'waley': 9,\n",
       " 'hongay': 2,\n",
       " 'warna': 15,\n",
       " 'shareef': 26,\n",
       " 'apni': 209,\n",
       " 'izat': 7,\n",
       " 'tamasha': 7,\n",
       " 'nah': 6,\n",
       " 'mujhay': 11,\n",
       " 'dusri': 3,\n",
       " 'dafa': 16,\n",
       " 'sacha': 3,\n",
       " 'pyaar': 5,\n",
       " 'gaya': 186,\n",
       " 'nai': 123,\n",
       " 'hazrat': 46,\n",
       " 'sahab': 32,\n",
       " 'dua': 133,\n",
       " 'darkast': 2,\n",
       " 'ha': 345,\n",
       " 'jaise': 24,\n",
       " 'shohar': 7,\n",
       " 'bano': 23,\n",
       " 'sabiq': 10,\n",
       " 'bete': 4,\n",
       " 'hamare': 20,\n",
       " 'ise': 11,\n",
       " 'aik': 374,\n",
       " 'sub': 55,\n",
       " 'standard': 2,\n",
       " 'mawad': 3,\n",
       " 'lekin': 130,\n",
       " 'un': 251,\n",
       " 'koi': 257,\n",
       " 'ibn': 9,\n",
       " 'safi': 10,\n",
       " 'zehan': 8,\n",
       " 'muqabla': 6,\n",
       " 'kar': 516,\n",
       " 'sakta': 53,\n",
       " 'tab': 15,\n",
       " 'sadabahar': 2,\n",
       " 'geet': 16,\n",
       " 'janam': 3,\n",
       " 'aye': 27,\n",
       " 'logo': 47,\n",
       " 'libas': 4,\n",
       " 'gair': 9,\n",
       " 'samne': 24,\n",
       " 'nikalne': 3,\n",
       " 'mana': 11,\n",
       " 'karo': 62,\n",
       " 'kya': 90,\n",
       " 'bt': 17,\n",
       " 'wo': 373,\n",
       " 'ashaar': 3,\n",
       " 'ilawa': 10,\n",
       " 'qadar': 12,\n",
       " 'pur': 23,\n",
       " 'kashish': 4,\n",
       " 'bana': 58,\n",
       " 'dete': 30,\n",
       " 'audience': 4,\n",
       " 'mashoor': 5,\n",
       " 'hue': 64,\n",
       " 'jate': 26,\n",
       " 'hahahah': 8,\n",
       " 'aa': 65,\n",
       " 'na': 438,\n",
       " 'zara': 14,\n",
       " '!': 123,\n",
       " 'judge': 25,\n",
       " 'karte': 55,\n",
       " 'kuch': 158,\n",
       " 'jab': 149,\n",
       " 'last': 16,\n",
       " 'sir': 55,\n",
       " 'us': 174,\n",
       " 'to': 650,\n",
       " 'write': 3,\n",
       " 'down': 2,\n",
       " 'these': 2,\n",
       " 'golden': 5,\n",
       " 'lines': 2,\n",
       " 'd': 72,\n",
       " 'aap': 175,\n",
       " 'sakht': 13,\n",
       " 'sadma': 2,\n",
       " 'howa': 36,\n",
       " 'kai': 43,\n",
       " 'roz': 28,\n",
       " 'tak': 158,\n",
       " 'dil': 101,\n",
       " 'rahe': 105,\n",
       " '.apni': 2,\n",
       " 'waldah': 13,\n",
       " 'wafat': 8,\n",
       " 'hawale': 23,\n",
       " 'iqbal': 39,\n",
       " 'maha': 2,\n",
       " 'raja': 8,\n",
       " 'tehrir': 9,\n",
       " 'h': 203,\n",
       " 'isnan': 2,\n",
       " 'kis': 29,\n",
       " 'taq': 2,\n",
       " 'bht': 141,\n",
       " 'acha': 175,\n",
       " 'ustad': 2,\n",
       " 'bare': 35,\n",
       " 'ghulam': 15,\n",
       " 'khan': 247,\n",
       " 'mosiqi': 6,\n",
       " 'tamam': 53,\n",
       " 'master': 10,\n",
       " 'inayat': 20,\n",
       " 'hussain': 57,\n",
       " 'par': 491,\n",
       " 'wa': 5,\n",
       " 'deye': 14,\n",
       " 'tawajah': 3,\n",
       " 'ese': 14,\n",
       " 'roshni': 4,\n",
       " 'jis': 158,\n",
       " 'izhar': 5,\n",
       " 'filmi': 9,\n",
       " 'tarah': 55,\n",
       " 'banaye': 5,\n",
       " 'howae': 19,\n",
       " 'amar': 8,\n",
       " 'aaj': 61,\n",
       " 'sune': 3,\n",
       " 'walon': 38,\n",
       " 'kanon': 2,\n",
       " 'ras': 2,\n",
       " 'gholte': 2,\n",
       " 'maza': 33,\n",
       " 'ata': 81,\n",
       " 'pehlay': 15,\n",
       " 'pagal': 20,\n",
       " 'phir': 91,\n",
       " 'pathar': 6,\n",
       " 'maaray': 2,\n",
       " 'lrki': 3,\n",
       " 'kitni': 8,\n",
       " 'buri': 17,\n",
       " 'lg': 13,\n",
       " 'rhi': 28,\n",
       " 'red': 4,\n",
       " 'dress': 6,\n",
       " 'song': 10,\n",
       " 'maqbool': 10,\n",
       " 'hua': 89,\n",
       " 'bolon': 3,\n",
       " 'suni': 6,\n",
       " 'ja': 80,\n",
       " 'sakti': 21,\n",
       " 'wesay': 3,\n",
       " 'pemra': 6,\n",
       " 'srif': 3,\n",
       " 'pti': 45,\n",
       " 'baqi': 19,\n",
       " 'kro': 38,\n",
       " 'pm': 14,\n",
       " 'sath': 222,\n",
       " 'her': 27,\n",
       " 'waqt': 55,\n",
       " 'hota': 106,\n",
       " 'jiska': 2,\n",
       " 'hotay': 11,\n",
       " 'bakwas': 19,\n",
       " 'ker': 103,\n",
       " 'rahi': 104,\n",
       " 'our': 22,\n",
       " 'business': 12,\n",
       " 'class': 20,\n",
       " 'first': 12,\n",
       " 'travel': 2,\n",
       " 'kerta': 4,\n",
       " 'shuhrat': 4,\n",
       " 'liay': 19,\n",
       " 'inho': 87,\n",
       " 'hukomat': 10,\n",
       " 'tamgha': 3,\n",
       " 'imtiaz': 7,\n",
       " 'hasil': 114,\n",
       " 'karne': 105,\n",
       " 'malika': 2,\n",
       " 'british': 30,\n",
       " 'life': 9,\n",
       " 'award': 23,\n",
       " 'tarikhi': 2,\n",
       " 'khawateen': 37,\n",
       " 'duniya': 60,\n",
       " 'badal': 6,\n",
       " 'di': 108,\n",
       " 'sikander': 3,\n",
       " 'ban': 61,\n",
       " 'haram': 18,\n",
       " 'khor': 5,\n",
       " 'mere': 82,\n",
       " 'liye': 242,\n",
       " 'yadgaar': 2,\n",
       " 'the': 224,\n",
       " 'film': 102,\n",
       " 'kabhi': 48,\n",
       " 'bhula': 2,\n",
       " 'yehi': 30,\n",
       " 'youth': 4,\n",
       " 'taqat': 6,\n",
       " 'laanat': 10,\n",
       " 'band': 65,\n",
       " 'be': 50,\n",
       " 'bus': 42,\n",
       " 'kharay': 3,\n",
       " 'jao': 17,\n",
       " 'kpk': 16,\n",
       " 'pata': 48,\n",
       " 'chal': 36,\n",
       " 'jayega': 3,\n",
       " 'hmari': 7,\n",
       " 'halat': 25,\n",
       " 'next': 19,\n",
       " 'andaza': 16,\n",
       " 'baat': 117,\n",
       " 'dehshat': 10,\n",
       " 'gardi': 6,\n",
       " 'waqeyat': 3,\n",
       " 'mukhtalif': 22,\n",
       " 'hadsat': 5,\n",
       " 'halak': 15,\n",
       " 'hone': 85,\n",
       " 'aza': 4,\n",
       " 'mutadid': 6,\n",
       " 'martaba': 21,\n",
       " 'apne': 155,\n",
       " 'hathon': 5,\n",
       " 'haan': 11,\n",
       " 'chor': 56,\n",
       " 'kehta': 10,\n",
       " 'bik': 9,\n",
       " 'auntie': 2,\n",
       " 'new': 25,\n",
       " 'episode': 19,\n",
       " 'nhi': 150,\n",
       " 'arhi': 3,\n",
       " 'q': 36,\n",
       " 'rana': 7,\n",
       " 'sahb': 8,\n",
       " 'lain': 7,\n",
       " 'kary': 29,\n",
       " 'kam': 101,\n",
       " 'apny': 44,\n",
       " 'say': 150,\n",
       " 'aisee': 2,\n",
       " 'paak': 7,\n",
       " 'rehmat': 10,\n",
       " '.m': 2,\n",
       " 'post': 16,\n",
       " 'p': 68,\n",
       " 'mention': 5,\n",
       " 'krny': 23,\n",
       " 'lgti': 3,\n",
       " '.pr': 3,\n",
       " 'pta': 33,\n",
       " 'galt': 4,\n",
       " 'jaty': 10,\n",
       " 'hn': 72,\n",
       " 'sha': 11,\n",
       " 'duaon': 16,\n",
       " 'shamil': 45,\n",
       " 'o': 149,\n",
       " 'tumhri': 2,\n",
       " 'gandi': 3,\n",
       " 'link': 2,\n",
       " 'ny': 70,\n",
       " 'delete': 2,\n",
       " 'hahaha': 50,\n",
       " 'choty': 4,\n",
       " 'baton': 4,\n",
       " 'uper': 10,\n",
       " 'zikar': 12,\n",
       " 'hon': 60,\n",
       " 'horaha': 5,\n",
       " 'es': 39,\n",
       " 'mulq': 2,\n",
       " 'afsos': 12,\n",
       " 'lol': 18,\n",
       " 'page': 6,\n",
       " 'waly': 29,\n",
       " 'dekh': 55,\n",
       " 'bat': 72,\n",
       " 'senate': 4,\n",
       " 'deputy': 2,\n",
       " 'gae': 40,\n",
       " 'isi': 39,\n",
       " 'kamiyabi': 19,\n",
       " 'mutassir': 8,\n",
       " 'steven': 9,\n",
       " 'spielberg': 5,\n",
       " 'banai': 9,\n",
       " 'box': 9,\n",
       " 'office': 8,\n",
       " 'ubqari': 12,\n",
       " 'khany': 2,\n",
       " 'mil': 55,\n",
       " 'jaye': 55,\n",
       " 'khazana': 11,\n",
       " 'mulk': 84,\n",
       " 'yahan': 33,\n",
       " 'baghair': 11,\n",
       " 'aise': 29,\n",
       " 'ijazat': 12,\n",
       " 'nahe': 32,\n",
       " 'win': 8,\n",
       " 'achi': 121,\n",
       " 'heat': 2,\n",
       " 'vote': 27,\n",
       " 'bajaye': 12,\n",
       " 'cnic': 3,\n",
       " 'dal': 15,\n",
       " 'dia': 48,\n",
       " 'shirkat': 13,\n",
       " 'shukrya': 2,\n",
       " 'jnb': 3,\n",
       " 'likh': 11,\n",
       " 'sakty': 11,\n",
       " 'han': 50,\n",
       " 'authority': 2,\n",
       " 'wise': 5,\n",
       " 'jari': 29,\n",
       " 'ak': 14,\n",
       " 'but': 44,\n",
       " 'mulazim': 4,\n",
       " 'samny': 3,\n",
       " 'karna': 68,\n",
       " 'galat': 12,\n",
       " 'national': 14,\n",
       " 'games': 6,\n",
       " 'championship': 2,\n",
       " 'jese': 21,\n",
       " 'muqablon': 4,\n",
       " 'saholaton': 2,\n",
       " 'mayar': 7,\n",
       " 'intehai': 7,\n",
       " 'kharab': 19,\n",
       " 'walid': 32,\n",
       " 'doston': 14,\n",
       " 'taqseem': 7,\n",
       " 'copy': 6,\n",
       " 'nizaam': 2,\n",
       " 'hyderabad': 4,\n",
       " 'bheji': 3,\n",
       " 'super': 17,\n",
       " 'match': 27,\n",
       " 'huay': 13,\n",
       " 'tumhen': 4,\n",
       " 'chahiye': 39,\n",
       " 'khabees': 15,\n",
       " 'gold': 6,\n",
       " '.i': 3,\n",
       " 'have': 11,\n",
       " 'always': 4,\n",
       " 'about': 5,\n",
       " 'it': 31,\n",
       " 'and': 76,\n",
       " 'today': 5,\n",
       " 'she': 5,\n",
       " 'has': 3,\n",
       " 'november': 14,\n",
       " 'inhein': 9,\n",
       " 'giraftar': 4,\n",
       " 'karliya': 5,\n",
       " 'seriously': 7,\n",
       " 'awam': 55,\n",
       " 'b': 306,\n",
       " 'punjab': 40,\n",
       " 'inter': 4,\n",
       " 'schools': 5,\n",
       " 'talba': 6,\n",
       " 'bari': 68,\n",
       " 'tadad': 19,\n",
       " 'sharik': 3,\n",
       " 'ab': 159,\n",
       " 'y': 21,\n",
       " 'masha': 22,\n",
       " 'allahallah': 4,\n",
       " 'lambi': 27,\n",
       " 'de': 97,\n",
       " 'aain': 6,\n",
       " 'mazhab': 5,\n",
       " 'hoqoq': 13,\n",
       " 'majida': 4,\n",
       " 'rizvi': 5,\n",
       " 'pori': 17,\n",
       " 'back': 7,\n",
       " 'ground': 3,\n",
       " 'awla': 4,\n",
       " 'test': 29,\n",
       " 'one': 33,\n",
       " 'day': 57,\n",
       " 'hat': 6,\n",
       " 'qalandars': 12,\n",
       " 'very': 27,\n",
       " 'mauqay': 2,\n",
       " 'per': 176,\n",
       " 'mukammal': 10,\n",
       " 'aman': 18,\n",
       " 'may': 38,\n",
       " 'tu': 162,\n",
       " 'hindustan': 16,\n",
       " 'bananay': 3,\n",
       " 'mashalla': 3,\n",
       " 'allha': 6,\n",
       " 'slamat': 5,\n",
       " 'rakhy': 33,\n",
       " 'ameen': 131,\n",
       " 'district': 2,\n",
       " 'nazim': 2,\n",
       " 'u': 44,\n",
       " 'c': 8,\n",
       " 'hal': 27,\n",
       " 'imran': 81,\n",
       " 'kaha': 72,\n",
       " 'zindagi': 75,\n",
       " 'insaf': 11,\n",
       " 'karkun': 8,\n",
       " 'khatra': 5,\n",
       " 'lahaq': 2,\n",
       " 'khun': 4,\n",
       " 'britannia': 3,\n",
       " 'hukumat': 32,\n",
       " 'muttahida': 2,\n",
       " 'qaumi': 11,\n",
       " 'movement': 3,\n",
       " 'london': 14,\n",
       " 'quaid': 24,\n",
       " 'altaf': 17,\n",
       " 'khalaf': 2,\n",
       " 'qadam': 10,\n",
       " 'uthaya': 7,\n",
       " 'shair': 4,\n",
       " 'karwai': 15,\n",
       " 'sher': 13,\n",
       " 'nhe': 16,\n",
       " 'agya': 3,\n",
       " 'ilzam': 12,\n",
       " 'gaali': 2,\n",
       " 'siasat': 5,\n",
       " 'lannat': 5,\n",
       " 'insan': 45,\n",
       " 'pe': 100,\n",
       " 'baita': 2,\n",
       " 'yai': 8,\n",
       " 'wrong': 7,\n",
       " 'side': 7,\n",
       " 'lga': 23,\n",
       " 'lia': 38,\n",
       " 'sonal': 2,\n",
       " 'telugu': 4,\n",
       " 'industry': 6,\n",
       " 'wabasta': 2,\n",
       " 'kerty': 2,\n",
       " 'huey': 8,\n",
       " 'legend': 2,\n",
       " 'naami': 3,\n",
       " 'sign': 4,\n",
       " 'li': 36,\n",
       " 'aksbandi': 6,\n",
       " 'dino': 11,\n",
       " 'tezi': 4,\n",
       " 'saath': 23,\n",
       " 'bharti': 15,\n",
       " 'blue': 2,\n",
       " 'asman': 3,\n",
       " 'kumar': 2,\n",
       " 'kaif': 3,\n",
       " 'sunjay': 2,\n",
       " 'dutt': 2,\n",
       " 'lara': 12,\n",
       " 'abraham': 2,\n",
       " 'role': 15,\n",
       " 'jaan': 26,\n",
       " 'nikal': 25,\n",
       " 'yaar': 35,\n",
       " 'buhat': 23,\n",
       " 'item': 9,\n",
       " 'use': 38,\n",
       " 'karunga': 4,\n",
       " 'tmhare': 2,\n",
       " 'lye': 34,\n",
       " 'best': 20,\n",
       " 'gehri': 3,\n",
       " 'classical': 4,\n",
       " 'awaaz': 16,\n",
       " 'gaane': 6,\n",
       " 'baqaida': 4,\n",
       " 'dawat': 4,\n",
       " 'sahara': 5,\n",
       " 'jany': 8,\n",
       " 'aksar': 7,\n",
       " 'hoty': 17,\n",
       " 'october': 10,\n",
       " 'unhain': 15,\n",
       " '.n': 2,\n",
       " '.o': 3,\n",
       " 'wafad': 3,\n",
       " 'bhaja': 3,\n",
       " 'gai': 146,\n",
       " 'zulfiqar': 5,\n",
       " 'bhutto': 22,\n",
       " 'russia': 8,\n",
       " 'america': 25,\n",
       " 'taraf': 58,\n",
       " 'silsilay': 5,\n",
       " 'bohot': 23,\n",
       " 'pesh': 42,\n",
       " 'boht': 46,\n",
       " 'aala': 14,\n",
       " 'saudi': 2,\n",
       " 'company': 6,\n",
       " 'arab': 9,\n",
       " 'power': 3,\n",
       " 'corporation': 2,\n",
       " 'khareed': 2,\n",
       " 'steel': 4,\n",
       " 'mills': 4,\n",
       " 'pia': 8,\n",
       " 'nijkari': 2,\n",
       " 'hogi': 25,\n",
       " 'bahawalpur': 2,\n",
       " 'ahmed': 27,\n",
       " 'faraz': 7,\n",
       " 'fun': 11,\n",
       " 'shakhsiyat': 22,\n",
       " 'unwaan': 2,\n",
       " 'phd': 2,\n",
       " 'maqala': 2,\n",
       " 'tehreer': 2,\n",
       " 'sada': 21,\n",
       " 'muskaraty': 2,\n",
       " 'g': 82,\n",
       " 'abay': 2,\n",
       " 'scha': 2,\n",
       " 'career': 18,\n",
       " 'mustaqbil': 8,\n",
       " 'faisla': 25,\n",
       " 'taleem': 32,\n",
       " 'bad': 182,\n",
       " 'karega': 2,\n",
       " 'bilkul': 84,\n",
       " 'achay': 16,\n",
       " 'mai': 103,\n",
       " 'yahi': 14,\n",
       " 'hna': 4,\n",
       " 'love': 37,\n",
       " 'siyasi': 24,\n",
       " 'samaji': 9,\n",
       " 'halqon': 7,\n",
       " 'janib': 23,\n",
       " 'miss': 14,\n",
       " 'haseena': 6,\n",
       " 'wajid': 7,\n",
       " 'mashwara': 2,\n",
       " 'diya': 133,\n",
       " 'apna': 79,\n",
       " 'zehni': 6,\n",
       " 'kyun': 43,\n",
       " 'qisam': 4,\n",
       " 'policy': 3,\n",
       " 'rakhi': 9,\n",
       " 'soorat': 7,\n",
       " 'mien': 54,\n",
       " 'qaum': 7,\n",
       " 'goth': 5,\n",
       " 'bais': 17,\n",
       " 'saala': 3,\n",
       " 'intaqal': 2,\n",
       " 'giya': 6,\n",
       " 'tasbih': 3,\n",
       " 'khane': 10,\n",
       " 'ky': 65,\n",
       " 'lajawb': 2,\n",
       " 'issue': 10,\n",
       " 'bhale': 3,\n",
       " 'mera': 84,\n",
       " 'cup': 20,\n",
       " 'karen': 35,\n",
       " 'beta': 20,\n",
       " 'bottle': 2,\n",
       " 'har': 89,\n",
       " 'karta': 49,\n",
       " 'as': 8,\n",
       " 'salam': 20,\n",
       " 'jumma': 4,\n",
       " 'mubarak': 46,\n",
       " 'this': 27,\n",
       " 'your': 20,\n",
       " 'eid': 48,\n",
       " 'lag': 40,\n",
       " 'rha': 44,\n",
       " 'pic': 22,\n",
       " 'lo': 48,\n",
       " 'see': 8,\n",
       " 'phr': 40,\n",
       " 'ami': 7,\n",
       " 'je': 6,\n",
       " 'pocho': 3,\n",
       " 'fori': 3,\n",
       " 'faiz': 9,\n",
       " 'inteqal': 14,\n",
       " 'tujy': 3,\n",
       " 'gy': 32,\n",
       " 'ty': 8,\n",
       " 'ajj': 7,\n",
       " 'dkh': 4,\n",
       " 'ly': 21,\n",
       " 'itnay': 6,\n",
       " 'baray': 11,\n",
       " 'jalsay': 4,\n",
       " 'woh': 89,\n",
       " 'pesa': 9,\n",
       " 'kahan': 20,\n",
       " 'sey': 77,\n",
       " 'khata': 3,\n",
       " 'hospital': 19,\n",
       " 'milti': 9,\n",
       " 'eman': 10,\n",
       " 'amaal': 4,\n",
       " 'wali': 83,\n",
       " 'zindge': 4,\n",
       " 'darkhast': 6,\n",
       " 'interview': 11,\n",
       " 'mujhe': 81,\n",
       " 'shohrat': 14,\n",
       " 'gumnami': 2,\n",
       " 'apki': 24,\n",
       " 'kaam': 121,\n",
       " 'thy': 26,\n",
       " 'physical': 2,\n",
       " 'comment': 9,\n",
       " 'krne': 22,\n",
       " 'guraiz': 3,\n",
       " 'kren': 20,\n",
       " 'agar': 93,\n",
       " 'muslem': 2,\n",
       " 'hyn': 11,\n",
       " 'ghar': 66,\n",
       " 'muslim': 36,\n",
       " 'samj': 2,\n",
       " 'stop': 7,\n",
       " 'dramatic': 2,\n",
       " 'acting': 28,\n",
       " 'saraha': 10,\n",
       " 'globe': 2,\n",
       " 'nominate': 2,\n",
       " 'hui': 63,\n",
       " 'belts': 2,\n",
       " 'quality': 71,\n",
       " 'mjhe': 27,\n",
       " 'inki': 16,\n",
       " 'shikayat': 5,\n",
       " 'munasib': 10,\n",
       " 'size': 23,\n",
       " 'sahi': 47,\n",
       " 'aya': 53,\n",
       " 'tw': 25,\n",
       " 'mutmaeen': 6,\n",
       " 'complain': 4,\n",
       " 'are': 8,\n",
       " 'not': 18,\n",
       " 'abhi': 32,\n",
       " 'rab': 21,\n",
       " 'bra': 3,\n",
       " 'kreem': 2,\n",
       " 'reham': 15,\n",
       " 'frma': 15,\n",
       " 'watn': 2,\n",
       " '.e': 2,\n",
       " 'sirf': 112,\n",
       " 'saal': 90,\n",
       " 'umer': 32,\n",
       " 'bengal': 2,\n",
       " 'council': 9,\n",
       " 'member': 4,\n",
       " 'bane': 9,\n",
       " 'faiyz': 2,\n",
       " 'reh': 18,\n",
       " 'musalmano': 13,\n",
       " 'huqooq': 4,\n",
       " 'khatir': 6,\n",
       " 'satah': 21,\n",
       " 'zaroor': 20,\n",
       " 'sharam': 11,\n",
       " 'ge': 42,\n",
       " 'family': 25,\n",
       " 'squash': 34,\n",
       " 'taweel': 12,\n",
       " 'kahna': 2,\n",
       " 'baja': 8,\n",
       " 'hoga': 45,\n",
       " 'haton': 4,\n",
       " 'kray': 7,\n",
       " 'doctor': 15,\n",
       " 'afia': 18,\n",
       " 'sadiqi': 2,\n",
       " 'refrendom': 2,\n",
       " 'aor': 13,\n",
       " 'sadiqiwazeer': 2,\n",
       " 'azam': 50,\n",
       " 'bnay': 3,\n",
       " 'aj': 49,\n",
       " 'kal': 39,\n",
       " 'hmri': 2,\n",
       " 'govt': 12,\n",
       " 'esi': 15,\n",
       " 'wja': 4,\n",
       " 'bethy': 4,\n",
       " 'tym': 3,\n",
       " '.logo': 2,\n",
       " 'khi': 7,\n",
       " 'rat': 10,\n",
       " 'baje': 5,\n",
       " 'double': 10,\n",
       " 'sawari': 2,\n",
       " 'din': 86,\n",
       " 'pabandi': 22,\n",
       " 'sindh': 36,\n",
       " 'teer': 3,\n",
       " 'kardia': 6,\n",
       " 'sehat': 18,\n",
       " 'farmaye': 21,\n",
       " 'orat': 8,\n",
       " 'guzishta': 9,\n",
       " 'december': 16,\n",
       " 'naye': 7,\n",
       " 'stadiums': 5,\n",
       " 'tameer': 11,\n",
       " 'purane': 2,\n",
       " 'leye': 95,\n",
       " 'brazil': 7,\n",
       " 'football': 12,\n",
       " 'assosiation': 4,\n",
       " 'brazilian': 4,\n",
       " 'lala': 12,\n",
       " 'great': 36,\n",
       " 'man': 16,\n",
       " 'lion': 2,\n",
       " 'hamain': 11,\n",
       " 'hajj': 5,\n",
       " 'sadat': 3,\n",
       " 'perfect': 4,\n",
       " 'naqsha': 2,\n",
       " 'khencha': 2,\n",
       " 'muj': 4,\n",
       " 'masoom': 10,\n",
       " 'inshaallah': 6,\n",
       " 'beshk': 4,\n",
       " 'chez': 10,\n",
       " 'qadeer': 2,\n",
       " 'yakeen': 5,\n",
       " 'krain': 5,\n",
       " 'wohi': 22,\n",
       " 'muaf': 2,\n",
       " 'saanp': 2,\n",
       " 'zahar': 3,\n",
       " 'huma': 5,\n",
       " 'pyare': 4,\n",
       " 'dr': 17,\n",
       " 'asim': 4,\n",
       " 'university': 18,\n",
       " 'record': 27,\n",
       " 'talab': 5,\n",
       " 'alla': 9,\n",
       " 'maqam': 9,\n",
       " 'mashi': 4,\n",
       " 'mushkilat': 7,\n",
       " 'khandan': 23,\n",
       " 'parvez': 5,\n",
       " 'musharraf': 17,\n",
       " 'aamriyat': 2,\n",
       " 'farzand': 2,\n",
       " ...}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 简单查看一下lang_process留下的单词\n",
    "lang_process.word2count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_pickle(lang_process, './data/lang_process.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将text转换为Tensor\n",
    "\n",
    "- 将text转换为Tensor(转换之前, 需要先要将单词进行标准化)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把data中的句子按顺序转为tensor\n",
    "# 这里转换的时候,句子中的单词也是需要标准化的\n",
    "def convertWord2index(word):\n",
    "    if lang_process.word2index.get(word)==None:\n",
    "        # 一些出现次数很少的词汇使用1来表示\n",
    "        return 1\n",
    "    else:\n",
    "        return lang_process.word2index.get(word)\n",
    "    \n",
    "input_tensor = [[convertWord2index(s) for s in normalizeString(es).split(' ')]  for es in data[\"review\"].values.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[192, 288, 1741, 239, 2065, 1552, 1, 4867, 169, 487, 5937, 181, 1552, 28],\n",
       " [5012,\n",
       "  1,\n",
       "  1,\n",
       "  3569,\n",
       "  5,\n",
       "  2386,\n",
       "  7113,\n",
       "  43,\n",
       "  1,\n",
       "  145,\n",
       "  1,\n",
       "  406,\n",
       "  49,\n",
       "  6395,\n",
       "  4930,\n",
       "  753,\n",
       "  1923,\n",
       "  84]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看最后两句话\n",
    "input_tensor[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ma na suna ha lemon sa haddiyan kamzor hoti hn regular Lana sa?',\n",
       " 'Ball poar jadooi giraft se inhe rafter aur swing ko qaboo karne ka hairat angez fun aata hai']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"review\"].values.tolist()[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ma'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这个index可以和word对应上(可以看到是忽略大小写的-这个很重要)\n",
    "lang_process.index2word[192]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Padding\n",
    "\n",
    "- 将Tensor转换为定长的Tensor, 多余的去掉, 不足的补0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.20480404551201\n",
      "12.0\n",
      "313\n"
     ]
    }
   ],
   "source": [
    "# 查看句子的平均长度, 长度的中位数, 最长的长度\n",
    "sentence_length = [len(t) for t in input_tensor]\n",
    "print(np.mean(sentence_length))\n",
    "print(np.median(sentence_length))\n",
    "print(np.max(sentence_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "bins = np.arange(0,300,10) # 产生区间刻度\n",
    "\n",
    "ax.hist(sentence_length,bins=bins)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "bins = np.arange(0,100,5) # 产生区间刻度\n",
    "\n",
    "ax.hist(sentence_length,bins=bins)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据上面的图像, 我们可以知道, 句子长度取80是一个比较好的值.\n",
    "\n",
    "即超过80个单词的句子去掉后面的部分, 少于80个单词的句子后面补充0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(x, max_len):\n",
    "    \"\"\"定义自动填充的函数\n",
    "    \"\"\"\n",
    "    padded = np.zeros((max_len), dtype=np.int64)\n",
    "    if len(x) > max_len: \n",
    "        padded[:] = x[:max_len]\n",
    "    else: \n",
    "        padded[:len(x)] = x\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = [pad_sequences(x, 80) for x in input_tensor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 192,  288, 1741,  239, 2065, 1552,    1, 4867,  169,  487, 5937,\n",
       "         181, 1552,   28,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0], dtype=int64),\n",
       " array([5012,    1,    1, 3569,    5, 2386, 7113,   43,    1,  145,    1,\n",
       "         406,   49, 6395, 4930,  753, 1923,   84,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0], dtype=int64)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看一下完成填充之后的数据\n",
    "input_tensor[-2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将Label(转换为one-hot的格式)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Negative', 'Positive'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看一下所有的分类\n",
    "data['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2emotion = {0: 'Negative', 1: 'Positive'}\n",
    "emotion2index = {'Negative' : 0, 'Positive' : 1}\n",
    "target_tensor = [emotion2index.get(s) for s in data['label'].values.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 0, 1, 0, 0, 1, 1, 0]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 简单查看一下样本\n",
    "target_tensor[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_pickle(input_tensor, './data/input_tensor.pkl')\n",
    "convert_to_pickle(target_tensor, './data/target_tensor.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到这里，数据集的预处理就已经全部处理好了，下面就是创建Data Loader, 最后可以用来放入整个模型中去."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader\n",
    "\n",
    "- 数据集的划分, 训练集和测试集\n",
    "- 数据集的加载, 使用DataLoader来加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5695, 5695, 633, 633)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 数据集的划分(这里全部是训练集)\n",
    "END = int(len(input_tensor)*0.9)\n",
    "\n",
    "# 将数据转为tensor的数据格式\n",
    "input_tensor_train = torch.from_numpy(np.array(input_tensor[:END]))\n",
    "target_tensor_train = torch.from_numpy(np.array(target_tensor[:END])).long()\n",
    "\n",
    "input_tensor_test = torch.from_numpy(np.array(input_tensor[END:]))\n",
    "target_tensor_test = torch.from_numpy(np.array(target_tensor[END:])).long()\n",
    "\n",
    "# Show length\n",
    "len(input_tensor_train), len(target_tensor_train), len(input_tensor_test), len(target_tensor_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = load_from_pickle('./data/test_loader.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 80]) torch.Size([64])\n",
      "torch.Size([64, 80]) torch.Size([64])\n",
      "torch.Size([64, 80]) torch.Size([64])\n",
      "torch.Size([64, 80]) torch.Size([64])\n",
      "torch.Size([64, 80]) torch.Size([64])\n",
      "torch.Size([64, 80]) torch.Size([64])\n",
      "torch.Size([64, 80]) torch.Size([64])\n",
      "torch.Size([64, 80]) torch.Size([64])\n",
      "torch.Size([64, 80]) torch.Size([64])\n",
      "torch.Size([57, 80]) torch.Size([57])\n"
     ]
    }
   ],
   "source": [
    "for i,j in test_loader:\n",
    "    print(i.shape,j.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载dataloader\n",
    "train_dataset = Data.TensorDataset(input_tensor_train, target_tensor_train) # 训练样本\n",
    "test_dataset = Data.TensorDataset(input_tensor_test, target_tensor_test) # 测试样本\n",
    "\n",
    "MINIBATCH_SIZE = 64\n",
    "\n",
    "train_loader = Data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=MINIBATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2           # set multi-work num read data\n",
    ")\n",
    "\n",
    "test_loader = Data.DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=MINIBATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2           # set multi-work num read data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_pickle(train_loader, './data/train_loader.pkl')\n",
    "convert_to_pickle(test_loader, './data/test_loader.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到这里，就创建好了dataload, 后面就可以开始构建网络，可以开始训练了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建GRU模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU的model\n",
    "class EmotionGRU(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_units, batch_sz, output_size, layers=2):\n",
    "        super(EmotionGRU, self).__init__()\n",
    "        self.vocab_size = vocab_size # 总的单词的个数\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_units = hidden_units\n",
    "        self.batch_sz = batch_sz\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = layers\n",
    "        \n",
    "        # layers\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim) # 可以将标号转为向量\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.gru = nn.GRU(self.embedding_dim, self.hidden_units, num_layers=self.num_layers, batch_first = True, bidirectional=True, dropout=0.5)\n",
    "        self.fc = nn.Linear(self.hidden_units*2, self.output_size)\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        # 使用了双向RNN, 所以num_layer*2\n",
    "        return torch.zeros((self.num_layers*2, self.batch_sz, self.hidden_units)).to(device)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        self.batch_sz = x.size(0)\n",
    "        # print(x.shape)\n",
    "        x = self.embedding(x)\n",
    "        # print(x.shape)\n",
    "        self.hidden = self.init_hidden()\n",
    "        output, self.hidden = self.gru(x, self.hidden)\n",
    "        # print(output.shape)\n",
    "        # 因为是 batch*seq*output, 所以要取最后一个seq\n",
    "        output = output[:,-1,:]\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc(output)\n",
    "        # print(output.shape)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试模型\n",
    "vocab_inp_size = len(lang_process.word2index)\n",
    "embedding_dim = 256\n",
    "hidden_units = 512\n",
    "target_size = 2 # 一共有2种emotion\n",
    "layers = 3\n",
    "\n",
    "# 测试模型\n",
    "model = EmotionGRU(vocab_inp_size, embedding_dim, hidden_units, MINIBATCH_SIZE, target_size, layers).to(device)\n",
    "\n",
    "# 测试数据\n",
    "it = iter(train_loader)\n",
    "x, y = next(it)\n",
    "\n",
    "output = model(x.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 2])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 64*2, 表示一共有64个样本, 每个样本是2个emotion的概率\n",
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(target, logit):\n",
    "    ''' Obtain accuracy for training round '''\n",
    "    target = torch.max(target, 1)[1] # convert from one-hot encoding to class indices\n",
    "    corrects = (logit == target).sum()\n",
    "    accuracy = 100.0 * corrects / len(logit)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(50, device='cuda:0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(output,y.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建LSTM模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM的model\n",
    "class EmotionLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_units, batch_sz, output_size, layers=2):\n",
    "        super(EmotionLSTM, self).__init__()\n",
    "        self.vocab_size = vocab_size # 总的单词的个数\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_units = hidden_units\n",
    "        self.batch_sz = batch_sz\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = layers\n",
    "        \n",
    "        # layers\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim) # 可以将标号转为向量\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.gru = nn.LSTM(self.embedding_dim, self.hidden_units, num_layers=self.num_layers, batch_first = True, bidirectional=True, dropout=0.5)\n",
    "        self.fc = nn.Linear(self.hidden_units*2, self.output_size)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # 使用了双向RNN, 所以num_layer*2\n",
    "        return (torch.zeros((self.num_layers*2, self.batch_sz, self.hidden_units)).to(device),\n",
    "               torch.zeros((self.num_layers*2, self.batch_sz, self.hidden_units)).to(device))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        self.batch_sz = x.size(0)\n",
    "        # print(x.shape)\n",
    "        x = self.embedding(x)\n",
    "        # print(x.shape)\n",
    "        (self.hidden, self.cell_state) = self.init_hidden()\n",
    "        output, (self.hidden, self.cell_state) = self.gru(x, (self.hidden, self.cell_state))\n",
    "        # print(output.shape)\n",
    "        # 因为是 batch*seq*output, 所以要取最后一个seq\n",
    "        output = output[:,-1,:]\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc(output)\n",
    "        # print(output.shape)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试模型\n",
    "vocab_inp_size = len(lang_process.word2index)\n",
    "embedding_dim = 256\n",
    "hidden_units = 512\n",
    "target_size = 2 # 一共有2种emotion\n",
    "layers = 3\n",
    "\n",
    "# 测试模型\n",
    "model = EmotionLSTM(vocab_inp_size, embedding_dim, hidden_units, MINIBATCH_SIZE, target_size, layers).to(device)\n",
    "\n",
    "# 测试数据\n",
    "it = iter(train_loader)\n",
    "x, y = next(it)\n",
    "\n",
    "output = model(x.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 2])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型\n",
    "- 定义损失函数\n",
    "- 定义优化器\n",
    "\n",
    "- 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Accuracy 51.0000. Loss 0.0109\n",
      "Epoch 1 Batch 25 Accuracy 50.0000. Loss 0.0130\n",
      "Epoch 1 Batch 50 Accuracy 49.1569. Loss 0.0121\n",
      "Epoch 1 Batch 75 Accuracy 49.7237. Loss 0.0117\n",
      "============\n",
      "Epoch 2 Batch 0 Accuracy 64.0000. Loss 0.0101\n",
      "Epoch 2 Batch 25 Accuracy 52.5000. Loss 0.0110\n",
      "Epoch 2 Batch 50 Accuracy 54.1569. Loss 0.0108\n",
      "Epoch 2 Batch 75 Accuracy 58.1842. Loss 0.0104\n",
      "============\n",
      "Epoch 3 Batch 0 Accuracy 75.0000. Loss 0.0073\n",
      "Epoch 3 Batch 25 Accuracy 73.3462. Loss 0.0085\n",
      "Epoch 3 Batch 50 Accuracy 73.8431. Loss 0.0084\n",
      "Epoch 3 Batch 75 Accuracy 73.6579. Loss 0.0083\n",
      "============\n",
      "Epoch 4 Batch 0 Accuracy 84.0000. Loss 0.0057\n",
      "Epoch 4 Batch 25 Accuracy 85.1154. Loss 0.0056\n",
      "Epoch 4 Batch 50 Accuracy 84.6078. Loss 0.0056\n",
      "Epoch 4 Batch 75 Accuracy 84.4211. Loss 0.0056\n",
      "============\n",
      "Epoch 5 Batch 0 Accuracy 82.0000. Loss 0.0057\n",
      "Epoch 5 Batch 25 Accuracy 91.6538. Loss 0.0033\n",
      "Epoch 5 Batch 50 Accuracy 91.0784. Loss 0.0034\n",
      "Epoch 5 Batch 75 Accuracy 91.0000. Loss 0.0034\n",
      "============\n",
      "Epoch 6 Batch 0 Accuracy 90.0000. Loss 0.0025\n",
      "Epoch 6 Batch 25 Accuracy 95.9231. Loss 0.0016\n",
      "Epoch 6 Batch 50 Accuracy 95.9020. Loss 0.0016\n",
      "Epoch 6 Batch 75 Accuracy 95.7368. Loss 0.0017\n",
      "============\n",
      "Epoch 7 Batch 0 Accuracy 100.0000. Loss 0.0003\n",
      "Epoch 7 Batch 25 Accuracy 97.8077. Loss 0.0008\n",
      "Epoch 7 Batch 50 Accuracy 97.5490. Loss 0.0009\n",
      "Epoch 7 Batch 75 Accuracy 97.5526. Loss 0.0009\n",
      "============\n",
      "Epoch 8 Batch 0 Accuracy 100.0000. Loss 0.0007\n",
      "Epoch 8 Batch 25 Accuracy 98.5769. Loss 0.0006\n",
      "Epoch 8 Batch 50 Accuracy 98.7059. Loss 0.0006\n",
      "Epoch 8 Batch 75 Accuracy 98.3816. Loss 0.0006\n",
      "============\n",
      "Epoch 9 Batch 0 Accuracy 100.0000. Loss 0.0001\n",
      "Epoch 9 Batch 25 Accuracy 99.0769. Loss 0.0003\n",
      "Epoch 9 Batch 50 Accuracy 99.1373. Loss 0.0003\n",
      "Epoch 9 Batch 75 Accuracy 99.1842. Loss 0.0004\n",
      "============\n",
      "Epoch 10 Batch 0 Accuracy 100.0000. Loss 0.0000\n",
      "Epoch 10 Batch 25 Accuracy 99.5385. Loss 0.0002\n",
      "Epoch 10 Batch 50 Accuracy 99.6471. Loss 0.0002\n",
      "Epoch 10 Batch 75 Accuracy 99.5526. Loss 0.0002\n",
      "============\n",
      "Epoch 11 Batch 0 Accuracy 100.0000. Loss 0.0001\n",
      "Epoch 11 Batch 25 Accuracy 99.6154. Loss 0.0001\n",
      "Epoch 11 Batch 50 Accuracy 99.6078. Loss 0.0001\n",
      "Epoch 11 Batch 75 Accuracy 99.6579. Loss 0.0001\n",
      "============\n",
      "Epoch 12 Batch 0 Accuracy 100.0000. Loss 0.0000\n",
      "Epoch 12 Batch 25 Accuracy 96.7308. Loss 0.0011\n",
      "Epoch 12 Batch 50 Accuracy 97.4314. Loss 0.0009\n",
      "Epoch 12 Batch 75 Accuracy 97.5263. Loss 0.0009\n",
      "============\n",
      "Epoch 13 Batch 0 Accuracy 100.0000. Loss 0.0003\n",
      "Epoch 13 Batch 25 Accuracy 99.2308. Loss 0.0003\n",
      "Epoch 13 Batch 50 Accuracy 99.0784. Loss 0.0004\n",
      "Epoch 13 Batch 75 Accuracy 99.0395. Loss 0.0004\n",
      "============\n",
      "Epoch 14 Batch 0 Accuracy 100.0000. Loss 0.0001\n",
      "Epoch 14 Batch 25 Accuracy 99.3846. Loss 0.0003\n",
      "Epoch 14 Batch 50 Accuracy 99.3333. Loss 0.0003\n",
      "Epoch 14 Batch 75 Accuracy 99.1711. Loss 0.0003\n",
      "============\n",
      "Epoch 15 Batch 0 Accuracy 100.0000. Loss 0.0000\n",
      "Epoch 15 Batch 25 Accuracy 98.5000. Loss 0.0006\n",
      "Epoch 15 Batch 50 Accuracy 98.9020. Loss 0.0004\n",
      "Epoch 15 Batch 75 Accuracy 98.9737. Loss 0.0004\n",
      "============\n",
      "Epoch 16 Batch 0 Accuracy 98.0000. Loss 0.0003\n",
      "Epoch 16 Batch 25 Accuracy 99.6923. Loss 0.0001\n",
      "Epoch 16 Batch 50 Accuracy 99.5294. Loss 0.0002\n",
      "Epoch 16 Batch 75 Accuracy 99.4737. Loss 0.0002\n",
      "============\n",
      "Epoch 17 Batch 0 Accuracy 100.0000. Loss 0.0000\n",
      "Epoch 17 Batch 25 Accuracy 99.6923. Loss 0.0001\n",
      "Epoch 17 Batch 50 Accuracy 99.6471. Loss 0.0001\n",
      "Epoch 17 Batch 75 Accuracy 99.7105. Loss 0.0001\n",
      "============\n",
      "Epoch 18 Batch 0 Accuracy 100.0000. Loss 0.0001\n",
      "Epoch 18 Batch 25 Accuracy 99.8462. Loss 0.0001\n",
      "Epoch 18 Batch 50 Accuracy 99.7255. Loss 0.0001\n",
      "Epoch 18 Batch 75 Accuracy 99.7368. Loss 0.0001\n",
      "============\n",
      "Epoch 19 Batch 0 Accuracy 100.0000. Loss 0.0000\n",
      "Epoch 19 Batch 25 Accuracy 99.7692. Loss 0.0001\n",
      "Epoch 19 Batch 50 Accuracy 99.7255. Loss 0.0001\n",
      "Epoch 19 Batch 75 Accuracy 99.8158. Loss 0.0001\n",
      "============\n",
      "Epoch 20 Batch 0 Accuracy 100.0000. Loss 0.0000\n",
      "Epoch 20 Batch 25 Accuracy 99.8462. Loss 0.0000\n",
      "Epoch 20 Batch 50 Accuracy 99.8039. Loss 0.0000\n",
      "Epoch 20 Batch 75 Accuracy 99.7632. Loss 0.0000\n",
      "============\n",
      "Epoch 21 Batch 0 Accuracy 100.0000. Loss 0.0000\n",
      "Epoch 21 Batch 25 Accuracy 100.0000. Loss 0.0000\n",
      "Epoch 21 Batch 50 Accuracy 99.8431. Loss 0.0001\n",
      "Epoch 21 Batch 75 Accuracy 99.8684. Loss 0.0000\n",
      "============\n",
      "Epoch 22 Batch 0 Accuracy 100.0000. Loss 0.0000\n",
      "Epoch 22 Batch 25 Accuracy 99.6923. Loss 0.0001\n",
      "Epoch 22 Batch 50 Accuracy 99.8431. Loss 0.0000\n",
      "Epoch 22 Batch 75 Accuracy 99.8684. Loss 0.0000\n",
      "============\n",
      "Epoch 23 Batch 0 Accuracy 100.0000. Loss 0.0000\n",
      "Epoch 23 Batch 25 Accuracy 99.9231. Loss 0.0000\n",
      "Epoch 23 Batch 50 Accuracy 99.8824. Loss 0.0000\n",
      "Epoch 23 Batch 75 Accuracy 99.8158. Loss 0.0001\n",
      "============\n",
      "Epoch 24 Batch 0 Accuracy 100.0000. Loss 0.0000\n",
      "Epoch 24 Batch 25 Accuracy 99.9231. Loss 0.0000\n",
      "Epoch 24 Batch 50 Accuracy 99.8431. Loss 0.0000\n",
      "Epoch 24 Batch 75 Accuracy 99.8684. Loss 0.0000\n",
      "============\n",
      "Epoch 25 Batch 0 Accuracy 100.0000. Loss 0.0000\n",
      "Epoch 25 Batch 25 Accuracy 99.8462. Loss 0.0001\n",
      "Epoch 25 Batch 50 Accuracy 99.8824. Loss 0.0000\n",
      "Epoch 25 Batch 75 Accuracy 99.8421. Loss 0.0001\n",
      "============\n"
     ]
    }
   ],
   "source": [
    "# 模型超参数\n",
    "vocab_inp_size = len(lang_process.word2index)\n",
    "embedding_dim = 256\n",
    "hidden_units = 512\n",
    "target_size = 2 # 一共有6种emotion\n",
    "num_layers = [1,2,3]\n",
    "\n",
    "for layers in num_layers:\n",
    "    # 测试模型\n",
    "    modelGRU = EmotionGRU(vocab_inp_size, embedding_dim, hidden_units, MINIBATCH_SIZE, target_size, layers).to(device)\n",
    "    modelLSTM = EmotionLSTM(vocab_inp_size, embedding_dim, hidden_units, MINIBATCH_SIZE, target_size, layers).to(device)\n",
    "    models = {'GRU':modelGRU, 'LSTM',modelLSTM}\n",
    "    for key, model in models.items():\n",
    "        # 定义损失函数和优化器\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
    "\n",
    "        # 开始训练\n",
    "        num_epochs = 25\n",
    "        for epoch in range(num_epochs):\n",
    "            start = time.time()\n",
    "\n",
    "            train_total_loss = 0 # 记录一整个epoch中的平均loss\n",
    "            train_total_accuracy = 0 # 记录一整个epoch中的平均accuracy\n",
    "\n",
    "            ### Training\n",
    "            for batch, (inp, targ) in enumerate(train_loader):\n",
    "                predictions = model(inp.to(device))  \n",
    "                # 计算误差     \n",
    "                loss = criterion(predictions, targ.to(device))\n",
    "                # 反向传播, 修改weight\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # 记录Loss下降和准确率的提升\n",
    "                ba   tch_loss = (loss / int(targ.size(0))) # 记录一个bacth的loss      \n",
    "                batch_accuracy = accuracy(predictions, targ.to(device))\n",
    "\n",
    "                train_total_loss = train_total_loss + batch_loss\n",
    "                train_total_accuracy = train_total_accuracy + batch_accuracy\n",
    "\n",
    "                if batch % 25 == 0:\n",
    "                    record_train_accuracy = train_total_accuracy.cpu().detach().numpy()/(batch+1)\n",
    "                    print('Epoch {} Batch {} Accuracy {:.4f}. Loss {:.4f}'.format(epoch + 1,\n",
    "                                                                 batch,\n",
    "                                                                 train_total_accuracy.cpu().detach().numpy()/(batch+1),\n",
    "                                                                 train_total_loss.cpu().detach().numpy()/(batch+1)))\n",
    "            # 每一个epoch来计算在test上的准确率\n",
    "            print('------------')\n",
    "            model.eval()\n",
    "            test_total_accuracy = 0\n",
    "            for batch, (input_data, target_data) in enumerate(test_loader):\n",
    "                predictions = model(input_data.to(device))\n",
    "                batch_accuracy = accuracy(predictions, target_data.to(device))\n",
    "                test_total_accuracy = test_total_accuracy + batch_accuracy\n",
    "            print('Test : Lay {}, Model {}, Epoch {} Accuracy {:.4f}'.format(layers, key, epoch + 1, test_total_accuracy.cpu().detach().numpy()/(batch+1)))\n",
    "            record_test_accuracy = test_total_accuracy.cpu().detach().numpy()/(batch+1)\n",
    "\n",
    "            if epoch == num_epochs - 1:\n",
    "                # 把最后一轮的结果写入文件\n",
    "                with open('byr.txt','a') as file:\n",
    "                    file.write('{},{},{:.4f},{:.4f}'.format(key,layers,record_train_accuracy,record_test_accuracy))\n",
    "            print('============')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "torch.save(model, 'EmotionRNN.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二分类问题计算AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([1, 1, 2, 2])\n",
    "pred = np.array([0.1, 0.4, 0.35, 0.8])\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=2)\n",
    "metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测新的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_csv('./data/20190610_test.csv',lineterminator='\\n',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = load_from_pickle('./data/20190610_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Sarcasm acha karti ho xD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Ali tareen ki kon c study yar...... ye bhai ph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Sharab Juwa Butt Aur Faal K Teer Shaitani Kam ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>salam hakeem sahab mne pore ramadan boht faida...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Bata ku nae dyty ake dusray ko apny dil ki baat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                             review\n",
       "0   1                           Sarcasm acha karti ho xD\n",
       "1   2  Ali tareen ki kon c study yar...... ye bhai ph...\n",
       "2   3  Sharab Juwa Butt Aur Faal K Teer Shaitani Kam ...\n",
       "3   4  salam hakeem sahab mne pore ramadan boht faida...\n",
       "4   5    Bata ku nae dyty ake dusray ko apny dil ki baat"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据存为pkl格式, 方便之后的使用\n",
    "convert_to_pickle(data_test, './data/20190610_test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试数据转换\n",
    "\n",
    "- 将测试数据转换为向量(文本向量话)\n",
    "- 将测试数据进行padding\n",
    "- 将测试数据转换为tenor(转换为可以放在Pytorch中进行训练的变量)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转换为tensor\n",
    "test_tensor = [[convertWord2index(s) for s in normalizeString(es).split(' ')]  for es in data_test[\"review\"].values.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 327, 189, 101, 3270],\n",
       " [77,\n",
       "  1595,\n",
       "  13,\n",
       "  2568,\n",
       "  640,\n",
       "  1,\n",
       "  127,\n",
       "  25,\n",
       "  25,\n",
       "  25,\n",
       "  25,\n",
       "  25,\n",
       "  25,\n",
       "  83,\n",
       "  46,\n",
       "  1435,\n",
       "  1848,\n",
       "  1,\n",
       "  1,\n",
       "  73,\n",
       "  25,\n",
       "  25,\n",
       "  25,\n",
       "  25,\n",
       "  83,\n",
       "  101,\n",
       "  1442,\n",
       "  114,\n",
       "  25,\n",
       "  25,\n",
       "  25,\n",
       "  25,\n",
       "  25,\n",
       "  1206,\n",
       "  1,\n",
       "  125,\n",
       "  6158,\n",
       "  5232,\n",
       "  101,\n",
       "  25,\n",
       "  25,\n",
       "  25,\n",
       "  4452,\n",
       "  1,\n",
       "  96,\n",
       "  710,\n",
       "  25,\n",
       "  25,\n",
       "  25,\n",
       "  1,\n",
       "  1,\n",
       "  202,\n",
       "  25,\n",
       "  25,\n",
       "  25,\n",
       "  4557,\n",
       "  141,\n",
       "  5218,\n",
       "  101,\n",
       "  28,\n",
       "  28,\n",
       "  28]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tensor[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ali', 'xd')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这个index可以和word对应上(可以看到是忽略大小写的-这个很重要)\n",
    "lang_process.index2word[77], lang_process.index2word[3270]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据进行padding\n",
    "test_tensor = [pad_sequences(x, 80) for x in test_tensor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([   1,  327,  189,  101, 3270,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0], dtype=int64),\n",
       " array([  77, 1595,   13, 2568,  640,    1,  127,   25,   25,   25,   25,\n",
       "          25,   25,   83,   46, 1435, 1848,    1,    1,   73,   25,   25,\n",
       "          25,   25,   83,  101, 1442,  114,   25,   25,   25,   25,   25,\n",
       "        1206,    1,  125, 6158, 5232,  101,   25,   25,   25, 4452,    1,\n",
       "          96,  710,   25,   25,   25,    1,    1,  202,   25,   25,   25,\n",
       "        4557,  141, 5218,  101,   28,   28,   28,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0], dtype=int64)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tensor[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把模型保存了\n",
    "convert_to_pickle(test_tensor, './data/test_tensor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据转为tensor的数据格式\n",
    "input_tensor_test = torch.from_numpy(np.array(test_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2712"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_tensor_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "MINIBATCH_SIZE = 64\n",
    "\n",
    "test_loader = Data.DataLoader(\n",
    "    dataset=input_tensor_test,\n",
    "    batch_size=MINIBATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=1          # set multi-work num read data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 进行预测\n",
    "\n",
    "- 模型的测试\n",
    "- test的测试结果输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载模型\n",
    "model = torch.load('EmotionRNN.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试数据\n",
    "it = iter(test_loader)\n",
    "x = next(it)\n",
    "\n",
    "model.eval()\n",
    "output = model(x.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 2])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.6093,  4.5610],\n",
       "        [ 5.9950, -4.9071]], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.8287e-05, 9.9996e-01],\n",
       "        [9.9998e-01, 1.8417e-05]], device='cuda:0', grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(output[:2],dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.8287195e-05, 9.9996173e-01],\n",
       "       [9.9998164e-01, 1.8417459e-05]], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(output[:2],dim=1).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmt='%f'控制保存为小数\n",
    "np.savetxt(\"test.csv\", F.softmax(output[:2],dim=1).cpu().detach().numpy(), delimiter=\",\", fmt='%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   1,  327,  189,  101, 3270,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [  77, 1595,   13, 2568,  640,    1,  127,   25,   25,   25,   25,   25,\n",
       "           25,   83,   46, 1435, 1848,    1,    1,   73,   25,   25,   25,   25,\n",
       "           83,  101, 1442,  114,   25,   25,   25,   25,   25, 1206,    1,  125,\n",
       "         6158, 5232,  101,   25,   25,   25, 4452,    1,   96,  710,   25,   25,\n",
       "           25,    1,    1,  202,   25,   25,   25, 4557,  141, 5218,  101,   28,\n",
       "           28,   28,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 确保输出的顺序是和之前的顺序是一样的\n",
    "x[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2712, 2)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_results = np.zeros((len(input_tensor_test),2))\n",
    "output_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 2)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_results[0:64].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "batch 20\n",
      "batch 40\n"
     ]
    }
   ],
   "source": [
    "# 所有数据的预测\n",
    "model.eval()\n",
    "\n",
    "output_results = np.zeros((len(input_tensor_test),2))\n",
    "\n",
    "with torch.no_grad():\n",
    "    start = 0\n",
    "    end = 0\n",
    "    for batch, input_data in enumerate(test_loader):\n",
    "        input_data = input_data.to(device)\n",
    "        outputs = model(input_data)\n",
    "        outputs = F.softmax(outputs,dim=1)\n",
    "        SIZE = outputs.size(0)\n",
    "        start = end\n",
    "        end = end + SIZE\n",
    "        output_results[start : end] = outputs.cpu().detach().numpy()\n",
    "        if batch% 20 == 0:\n",
    "            print(\"batch {}\".format(batch))\n",
    "\n",
    "# 将结果保存在csv中\n",
    "np.savetxt(\"backup.csv\", output_results, delimiter=\",\", fmt='%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
